{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 基于PaddleNLP的28“微”情感多标签分类实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一.项目介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 项目简介\n",
    "\n",
    "文本情感分析作为自然语言处理中主要研究问题之一，其利用自然语言处理和文本挖掘技术，对带有情感色彩的主观性文本进行分析、处理、归纳和推理，最终得出情感类别。\n",
    "\n",
    "情感分析通过对文本信息的情感倾向判断挖掘内容潜在价值，不仅有利于管理部门的监控，而且对于谣言制止、舆情导向、市场营销等都具有非常高的应用价值，受到了工业界、商业界还是学术界极高的关注。\n",
    "\n",
    "\n",
    "在情感分析中，粒度与准确度是影响情感分析效果的重要因素。目前大多数现有情感分析系统采用的正面、中性、负面的情感分类方式，其较难表达人类情感的复杂性，难以挖掘文本中潜在情感。2020年ACL会议上Google研究员发布了迄今为止最大、情感粒度最细的微情绪人工标注数据集GoEmotions，包含了58000个人工标注的Reddit 评论，情绪类别首次提升到28种，为更好挖掘用户潜在情感提供了契机。\n",
    "\n",
    "本项目将通过预训练模型Erinie3.0在处理后的微情感多分类数据集GoEmotions上进行微调训练，构建精细化微情感多分类模型，细化情感分析结果粒度同时优化微情感分类效果。\n",
    "\n",
    "## 1.2 项目难点\n",
    "\n",
    "GoEmotions微情感多分类数据集不同于传统情感2-7分类，其首次将情感粒度提升至28种更为精细化。不同于简单的单分类任务，GoEmotions数据集中一条句子可能对应2种甚至3种情感，属于文本多标签分类任务。情感粒度的精细化以及多标签分类是本项目一大难点。本项目将介绍如何基于PaddleNLP对ERNIE 3.0预训练模型微调完成GoEmotions微情感文本多标签分类预测。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/6a84bad7a8c546918528430543dac9bc72d1f82721de48b08e881fa8fec3979b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二.GoEmotions 微情感28多分类数据集\n",
    "\n",
    "**GoEmotions论文（推荐阅读）：**[https://aclanthology.org/2020.acl-main.372.pdf](https://aclanthology.org/2020.acl-main.372.pdf)\n",
    "\n",
    "让机器理解情境和情感一直是研究界的一个长期目标。2020年ACL会议上Google研究员发布了迄今为止最大、情感粒度最细的微情绪人工标注数据集GoEmotions。其包含了58000个人工标注的Reddit 评论，情绪类别首次提升到28种。与基本的六种情绪(只有一种积极情绪(喜悦))不同，其分类如图所示，包括12种积极情绪、11种消极情绪、4种模糊情绪和1种“中性”情绪，这使得它广泛适用于需要微妙区分情绪表达的情绪理解任务，更好挖掘用用户潜在情感，具有很大研究意义。\n",
    "\n",
    "28种微情感类别及整体分布情况：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a29f024daafb4d69ac46587fdc05f68dd63df17ddd254f17a5dc9aaa92d44e41)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/2bf9315150294f68abf09270217ad4e44df26aeb812f4e00811335d310350c2e)\n",
    "\n",
    "为了最大程度的减少数据中的噪音，过滤掉数据集中仅由一个注释者选择的情感标签。执行此过滤后，将数据随机划分为训练（90%）和测试集（10%）。\n",
    "\n",
    "在数据集中每一行包括两部分内容：句子内容、句子对应情感类型，使用‘\\t’分隔符分隔。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:14:57.368402Z",
     "iopub.status.busy": "2022-11-16T07:14:57.367717Z",
     "iopub.status.idle": "2022-11-16T07:14:57.375131Z",
     "shell.execute_reply": "2022-11-16T07:14:57.374110Z",
     "shell.execute_reply.started": "2022-11-16T07:14:57.368366Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/data/data177486\r\n"
     ]
    }
   ],
   "source": [
    "# 进入数据集所在文件路径\n",
    "%cd /home/aistudio/data/data177486/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:14:59.190279Z",
     "iopub.status.busy": "2022-11-16T07:14:59.189707Z",
     "iopub.status.idle": "2022-11-16T07:14:59.427667Z",
     "shell.execute_reply": "2022-11-16T07:14:59.426583Z",
     "shell.execute_reply.started": "2022-11-16T07:14:59.190247Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "# 查看路径下文件：训练集train.csv  测试集 test.csv\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:15:01.023127Z",
     "iopub.status.busy": "2022-11-16T07:15:01.022507Z",
     "iopub.status.idle": "2022-11-16T07:15:01.650554Z",
     "shell.execute_reply": "2022-11-16T07:15:01.649526Z",
     "shell.execute_reply.started": "2022-11-16T07:15:01.023089Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pandas读取数据集便于分析\n",
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv', sep='\\t', header=None)\n",
    "test = pd.read_csv('test.csv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:15:03.454370Z",
     "iopub.status.busy": "2022-11-16T07:15:03.453269Z",
     "iopub.status.idle": "2022-11-16T07:15:03.458622Z",
     "shell.execute_reply": "2022-11-16T07:15:03.457737Z",
     "shell.execute_reply.started": "2022-11-16T07:15:03.454332Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 添加列名\n",
    "train.columns = [\"text\",'labels']\n",
    "test.columns = [\"text\",'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:15:05.602086Z",
     "iopub.status.busy": "2022-11-16T07:15:05.601414Z",
     "iopub.status.idle": "2022-11-16T07:15:05.622794Z",
     "shell.execute_reply": "2022-11-16T07:15:05.622014Z",
     "shell.execute_reply.started": "2022-11-16T07:15:05.602042Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yes I heard abt the f bombs! That has to be wh...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We need more boards and to create a bit more s...</td>\n",
       "      <td>8,20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Damn youtube and outrage drama is super lucrat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It might be linked to the trust factor of your...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0  My favourite food is anything I didn't have to...     27\n",
       "1  Now if he does off himself, everyone will thin...     27\n",
       "2                     WHY THE FUCK IS BAYLESS ISOING      2\n",
       "3                        To make her feel threatened     14\n",
       "4                             Dirty Southern Wankers      3\n",
       "5  OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...     26\n",
       "6  Yes I heard abt the f bombs! That has to be wh...     15\n",
       "7  We need more boards and to create a bit more s...   8,20\n",
       "8  Damn youtube and outrage drama is super lucrat...      0\n",
       "9  It might be linked to the trust factor of your...     27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取训练集前10条文本，数据格式为text, labels。注意labels可能包含多个情感类别，各情感使用','进行分隔\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:15:15.429691Z",
     "iopub.status.busy": "2022-11-16T07:15:15.428880Z",
     "iopub.status.idle": "2022-11-16T07:15:15.439692Z",
     "shell.execute_reply": "2022-11-16T07:15:15.438923Z",
     "shell.execute_reply.started": "2022-11-16T07:15:15.429642Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m really sorry about your situation :( Altho...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's wonderful because it's awful. At not with.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kings fan here, good luck to you guys! Will be...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I didn't know that, thank you for teaching me ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They got bored from haunting earth for thousan...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thank you for asking questions and recognizing...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You’re welcome</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100%! Congrats on your job too!</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I’m sorry to hear that friend :(. It’s for the...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Girlfriend weak as well, that jump was pathetic.</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[NAME] has towed the line of the Dark Side. He...</td>\n",
       "      <td>3,10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lol! But I love your last name though. XD</td>\n",
       "      <td>1,18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Translation }}} I wish I could afford it.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>It's great that you're a recovering addict, th...</td>\n",
       "      <td>0,7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I've also heard that intriguing but also kinda...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text labels\n",
       "0   I’m really sorry about your situation :( Altho...     25\n",
       "1     It's wonderful because it's awful. At not with.      0\n",
       "2   Kings fan here, good luck to you guys! Will be...     13\n",
       "3   I didn't know that, thank you for teaching me ...     15\n",
       "4   They got bored from haunting earth for thousan...     27\n",
       "5   Thank you for asking questions and recognizing...     15\n",
       "6                                      You’re welcome     15\n",
       "7                     100%! Congrats on your job too!     15\n",
       "8   I’m sorry to hear that friend :(. It’s for the...     24\n",
       "9    Girlfriend weak as well, that jump was pathetic.     25\n",
       "10  [NAME] has towed the line of the Dark Side. He...   3,10\n",
       "11          Lol! But I love your last name though. XD   1,18\n",
       "12          Translation }}} I wish I could afford it.      8\n",
       "13  It's great that you're a recovering addict, th...    0,7\n",
       "14  I've also heard that intriguing but also kinda...     14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三.基于PaddleNLP搭建微情感28多分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 PaddleNLP依赖导入\n",
    "\n",
    "PaddleNLP 是飞桨自然语言处理开发库，具备 易用的文本领域API，多场景的应用示例、和高性能分布式训练三大特点，旨在提升飞桨开发者文本领域建模效率，旨在提升开发者在文本领域的开发效率，并提供丰富的NLP应用示例。\n",
    "\n",
    "**1.易用的文本领域API：**\n",
    "\n",
    "提供丰富的产业级预置任务能力 Taskflow 和全流程的文本领域API：支持丰富中文数据集加载的 Dataset API，可灵活高效地完成数据预处理的 Data API ，预置60+预训练词向量的 Embedding API ，提供100+预训练模型的 Transformer API 等，可大幅提升NLP任务建模的效率。\n",
    "\n",
    "**2.多场景的应用示例：**\n",
    "\n",
    "覆盖从学术到产业级的NLP应用示例，涵盖NLP基础技术、NLP系统应用以及相关拓展应用。全面基于飞桨核心框架2.0全新API体系开发，为开发者提供飞桨文本领域的最佳实践。\n",
    "\n",
    "**3.高性能分布式训练：**\n",
    "\n",
    "基于飞桨核心框架领先的自动混合精度优化策略，结合分布式Fleet API，支持4D混合并行策略，可高效地完成大规模预训练模型训练。\n",
    "\n",
    "**项目GitHub:** [https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "\n",
    "**项目Gitee:** [https://gitee.com/paddlepaddle/PaddleNLP](https://gitee.com/paddlepaddle/PaddleNLP)\n",
    "\n",
    "**GitHub Issue反馈:** [https://github.com/PaddlePaddle/PaddleNLP/issues](https://github.com/PaddlePaddle/PaddleNLP/issues)\n",
    "\n",
    "**PaddleNLP文档：** [https://paddlenlp.readthedocs.io/zh/latest/index.html](https://paddlenlp.readthedocs.io/zh/latest/index.html)\n",
    "\n",
    "**PaddleNLP支持预训练模型汇总**：[https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/e2c8eb213fcd4d2eaedc03f53842200456afbca266034c9f813454fa68e7d58d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:15:40.776529Z",
     "iopub.status.busy": "2022-11-16T07:15:40.775929Z",
     "iopub.status.idle": "2022-11-16T07:15:43.611032Z",
     "shell.execute_reply": "2022-11-16T07:15:43.609148Z",
     "shell.execute_reply.started": "2022-11-16T07:15:40.776494Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddlenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 数据预处理\n",
    "\n",
    "对于28微情感多标签分类场景，即一个句子可能对应多个情感类别标签, 首先需要对数据集情感标签使用**One-Hot**编码进行转换，对于每种情感0表示不存在而1表示存在。\n",
    "\n",
    "使用本地文件创建数据集，自定义read_custom_data()函数读取数据文件，传入load_dataset()创建数据集，返回数据类型为MapDataset。更多数据集自定方法详见[如何自定义数据集](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_self_defined.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:16:57.651039Z",
     "iopub.status.busy": "2022-11-16T07:16:57.650384Z",
     "iopub.status.idle": "2022-11-16T07:16:57.656962Z",
     "shell.execute_reply": "2022-11-16T07:16:57.656213Z",
     "shell.execute_reply.started": "2022-11-16T07:16:57.650996Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 28类微情感映射关系\n",
    "label_vocab = {\n",
    "    0: \"admiration\",\n",
    "    1: \"amusement\",\n",
    "    2: \"anger\",\n",
    "    3: \"annoyance\",\n",
    "    4: \"approval\",\n",
    "    5: \"caring\",\n",
    "    6: \"confusion\",\n",
    "    7: \"curiosity\",\n",
    "    8: \"desire\",\n",
    "    9: \"disappointment\",\n",
    "    10: \"disapproval\",\n",
    "    11: \"disgust\",\n",
    "    12: \"embarrassment\",\n",
    "    13: \"excitement\",\n",
    "    14: \"fear\",\n",
    "    15: \"gratitude\",\n",
    "    16: \"grief\",\n",
    "    17: \"joy\",\n",
    "    18: \"love\",\n",
    "    19: \"nervousness\",\n",
    "    20: \"optimism\",\n",
    "    21: \"pride\",\n",
    "    22: \"realization\",\n",
    "    23: \"relief\",\n",
    "    24: \"remorse\",\n",
    "    25: \"sadness\",\n",
    "    26: \"surprise\",\n",
    "    27: \"neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:16:59.819643Z",
     "iopub.status.busy": "2022-11-16T07:16:59.818995Z",
     "iopub.status.idle": "2022-11-16T07:16:59.827452Z",
     "shell.execute_reply": "2022-11-16T07:16:59.826500Z",
     "shell.execute_reply.started": "2022-11-16T07:16:59.819596Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 自定义数据集\n",
    "import re\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "# 清洗无效字符\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "    text = re.sub(r\"\\\\n\\n\", \".\", text)\n",
    "    return text\n",
    "\n",
    "# 定义读取数据集函数\n",
    "def read_custom_data(filepath, is_one_hot=True):\n",
    "    f = open(filepath)\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        data = line.strip().split('\\t')\n",
    "        # 针对28类微情感标签做One-hot处理\n",
    "        if is_one_hot:\n",
    "            labels = [float(1) if str(i) in data[1].split(',') else float(0) for i in range(28)]  # 28类\n",
    "        else:\n",
    "            labels = [int(d) for d in data[1].split(',')]\n",
    "        yield {\"text\": clean_text(data[0]), \"labels\": labels}\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:17:02.761528Z",
     "iopub.status.busy": "2022-11-16T07:17:02.760659Z",
     "iopub.status.idle": "2022-11-16T07:17:03.851015Z",
     "shell.execute_reply": "2022-11-16T07:17:03.849702Z",
     "shell.execute_reply.started": "2022-11-16T07:17:02.761486Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load_dataset()创建数据集\n",
    "# lazy=False，数据集返回为MapDataset类型\n",
    "# 对训练集和验证集进行预处理\n",
    "train_ds = load_dataset(read_custom_data, filepath='train.csv', lazy=False) \n",
    "test_ds = load_dataset(read_custom_data, filepath='test.csv', lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:17:05.795286Z",
     "iopub.status.busy": "2022-11-16T07:17:05.794644Z",
     "iopub.status.idle": "2022-11-16T07:17:05.807194Z",
     "shell.execute_reply": "2022-11-16T07:17:05.806441Z",
     "shell.execute_reply.started": "2022-11-16T07:17:05.795243Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据类型: <class 'paddlenlp.datasets.dataset.MapDataset'>\r\n",
      "训练集样例: {'text': \"My favourite food is anything I didn't have to cook myself.\", 'labels': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}\r\n",
      "测试集样例: {'text': 'I’m really sorry about your situation :( Although I love the names Sapphira, Cirilla, and Scarlett!', 'labels': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}\r\n"
     ]
    }
   ],
   "source": [
    "print(\"数据类型:\", type(train_ds))\n",
    "\n",
    "# labels为One-hot标签\n",
    "print(\"训练集样例:\", train_ds[0])\n",
    "print(\"测试集样例:\", test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 加载预训练模型 ERNIE 3.0\n",
    "\n",
    "ERNIE 3.0首次在百亿级预训练模型中引入大规模知识图谱，提出了海量无监督文本与大规模知识图谱的平行预训练方法(Universal Knowledge-Text Prediction)，通过将知识图谱挖掘算法得到五千万知识图谱三元组与4TB大规模语料同时输入到预训练模型中进行联合掩码训练，促进了结构化知识和无结构文本之间的信息共享，大幅提升了模型对于知识的记忆和推理能力。\n",
    "\n",
    "ERNIE 3.0框架分为两层。第一层是通用语义表示网络，该网络学习数据中的基础和通用的知识。第二层是任务语义表示网络，该网络基于通用语义表示，学习任务相关的知识。在学习过程中，任务语义表示网络只学习对应类别的预训练任务，而通用语义表示网络会学习所有的预训练任务。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a3cc2adb659e46d5897453a892d92c34fcaec6805d4e47c1b86e933661416c5f)\n",
    "\n",
    "\n",
    "PaddleNLP中Auto模块（包括AutoModel, AutoTokenizer及各种下游任务类）提供了方便易用的接口，无需指定模型类别，即可调用不同网络结构的预训练模型。PaddleNLP的预训练模型可以很容易地通过from_pretrained()方法加载，Transformer预训练模型汇总包含了40多个主流预训练模型，500多个模型权重。\n",
    "\n",
    "AutoModelForSequenceClassification可用于多标签分类，通过预训练模型获取输入文本的表示，之后将文本表示进行分类。\n",
    "\n",
    "PaddleNLP已经实现了ERNIE 3.0预训练模型，可以通过一行代码实现ERNIE 3.0预训练模型和分词器的加载。\n",
    "\n",
    "**PaddleNLP支持的更多预训练模型：**[https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:17:25.871086Z",
     "iopub.status.busy": "2022-11-16T07:17:25.869552Z",
     "iopub.status.idle": "2022-11-16T07:18:07.111622Z",
     "shell.execute_reply": "2022-11-16T07:18:07.110621Z",
     "shell.execute_reply.started": "2022-11-16T07:17:25.871007Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-11-16 15:17:25,874] [    INFO] - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-medium-zh'.\r\n",
      "[2022-11-16 15:17:25,878] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-3.0-medium-zh\r\n",
      "[2022-11-16 15:17:25,881] [    INFO] - Downloading ernie_3.0_medium_zh.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh.pdparams\r\n",
      "100%|██████████| 313M/313M [00:37<00:00, 8.66MB/s] \r\n",
      "W1116 15:18:03.879670   259 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\r\n",
      "W1116 15:18:03.884307   259 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\r\n",
      "[2022-11-16 15:18:06,760] [    INFO] - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\r\n",
      "[2022-11-16 15:18:06,764] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh_vocab.txt and saved to /home/aistudio/.paddlenlp/models/ernie-3.0-medium-zh\r\n",
      "[2022-11-16 15:18:06,766] [    INFO] - Downloading ernie_3.0_medium_zh_vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh_vocab.txt\r\n",
      "100%|██████████| 182k/182k [00:00<00:00, 936kB/s] \r\n",
      "[2022-11-16 15:18:07,102] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json\r\n",
      "[2022-11-16 15:18:07,105] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json\r\n"
     ]
    }
   ],
   "source": [
    "# 加载中文ERNIE 3.0预训练模型和分词器\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"ernie-3.0-medium-zh\"   # ERNIE3.0 模型\n",
    "num_classes = 28  # 28分类任务\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 数据处理为模型可接受格式\n",
    "\n",
    "Dataset中通常为原始数据，需要经过一定的数据处理并进行采样组batch：通过Dataset的map函数，使用分词器将数据集从原始文本处理成模型的输入。定义paddle.io.BatchSampler和collate_fn构建 paddle.io.DataLoader。\n",
    "\n",
    "实际训练中，根据显存大小调整批大小batch_size和文本最大长度max_seq_length。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:18:16.013477Z",
     "iopub.status.busy": "2022-11-16T07:18:16.012809Z",
     "iopub.status.idle": "2022-11-16T07:18:16.021358Z",
     "shell.execute_reply": "2022-11-16T07:18:16.020520Z",
     "shell.execute_reply.started": "2022-11-16T07:18:16.013434Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "\n",
    "# 数据预处理函数，利用分词器将文本转化为整数序列\n",
    "def preprocess_function(examples, tokenizer, max_seq_length):\n",
    "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    return result\n",
    "\n",
    "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=64)\n",
    "train_ds = train_ds.map(trans_func)\n",
    "test_ds = test_ds.map(trans_func)\n",
    "\n",
    "# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "test_batch_sampler = BatchSampler(test_ds, batch_size=16, shuffle=False)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 定义模型验证指标\n",
    "\n",
    "准确率作为实验最常用的评估指标，但GoEmotions数据中存在样本较大不均衡的情况时，此时使用准确率的话并不能合理反映模型的预测能力。将28种微情感模型OneHot化为0和1后，其中0代表句子中不存在该情感，而1代表存在。对于数量较少的情感，其负样本过多远远超过正样本，假设模型全预测为负样本0此时仍然能达到较高的准确率，模型对正样本将失去识别能力，高准确率不能反映模型的预测能力。因而不适合使用准确率指标，更适合使用Precision、Recall和F1-Score指标对多分类进行综合评估。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/bc39040c2e2b4864ac1a4d76e3886530ce700113138f4a31a5adfe05d4daa195)\n",
    "\n",
    "Precision是针对预测结果而言的。预测结果中，预测为正的样本中预测正确的概率，类似于一个考生在考卷上写出来的答案中，正确了多少，体现模型的精准度。而Recall表示实际为正的样本被判断为正样本的比例，类似于一个考生在考卷上回答了多少题。体现一个模型的全面性。Precision和Recall是一对矛盾的度量，一般来说，Precision高时，Recall值往往偏低；而Precision值低时，Recall值往往偏高。\n",
    "\n",
    "F1的核心思想在于，在尽可能的提高Precision和Recall的同时，也希望两者之间的差异尽可能小，可以综合考虑两大指标。F1-score适用于二分类问题，对于多分类问题，将二分类的F1-score推广，有Micro-F1和Macro-F1两种度量。Macro-average统计各个类别的TP、FP、FN、TN，分别计算各自的Precision和Recall，得到各自的F1值，然后取平均值得到Macro-F1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:18:21.008670Z",
     "iopub.status.busy": "2022-11-16T07:18:21.008057Z",
     "iopub.status.idle": "2022-11-16T07:18:21.057912Z",
     "shell.execute_reply": "2022-11-16T07:18:21.056884Z",
     "shell.execute_reply.started": "2022-11-16T07:18:21.008632Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from paddle.metric import Metric\n",
    "\n",
    "# 自定义MultiLabelReport评价指标\n",
    "class MultiLabelReport(Metric):\n",
    "    \"\"\"\n",
    "    AUC and F1 Score for multi-label text classification task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='MultiLabelReport', average='micro'):\n",
    "        super(MultiLabelReport, self).__init__()\n",
    "        self.average = average\n",
    "        self._name = name\n",
    "        self.reset()\n",
    "\n",
    "    def f1_score(self, y_prob):\n",
    "        '''\n",
    "        Returns the f1 score by searching the best threshhold\n",
    "        '''\n",
    "        best_score = 0\n",
    "        for threshold in [i * 0.01 for i in range(100)]:\n",
    "            self.y_pred = y_prob > threshold\n",
    "            score = sklearn.metrics.f1_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                precison = precision_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "                recall = recall_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "        return best_score, precison, recall\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state.\n",
    "        \"\"\"\n",
    "        self.y_prob = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def update(self, probs, labels):\n",
    "        if self.y_prob is not None:\n",
    "            self.y_prob = np.append(self.y_prob, probs.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_prob = probs.numpy()\n",
    "        if self.y_true is not None:\n",
    "            self.y_true = np.append(self.y_true, labels.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_true = labels.numpy()\n",
    "\n",
    "    def accumulate(self):\n",
    "        auc = roc_auc_score(\n",
    "            y_score=self.y_prob, y_true=self.y_true, average=self.average)\n",
    "        f1_score, precison, recall = self.f1_score(y_prob=self.y_prob)\n",
    "        return auc, f1_score, precison, recall\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns metric name\n",
    "        \"\"\"\n",
    "        return self._name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 选择优化策略和运行配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:18:24.922553Z",
     "iopub.status.busy": "2022-11-16T07:18:24.921880Z",
     "iopub.status.idle": "2022-11-16T07:18:24.928542Z",
     "shell.execute_reply": "2022-11-16T07:18:24.927753Z",
     "shell.execute_reply.started": "2022-11-16T07:18:24.922507Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# AdamW优化器、交叉熵损失函数、自定义MultiLabelReport评价指标\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=4e-5, parameters=model.parameters(), weight_decay=0.01)\n",
    "criterion = paddle.nn.BCEWithLogitsLoss()\n",
    "metric = MultiLabelReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 模型训练和验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:18:26.719334Z",
     "iopub.status.busy": "2022-11-16T07:18:26.718676Z",
     "iopub.status.idle": "2022-11-16T07:18:26.729305Z",
     "shell.execute_reply": "2022-11-16T07:18:26.728386Z",
     "shell.execute_reply.started": "2022-11-16T07:18:26.719291Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# 构建验证集evaluate函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader, label_vocab, if_return_results=True):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    results = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.sigmoid(logits)\n",
    "        losses.append(loss.numpy())\n",
    "        metric.update(probs, labels)\n",
    "        if if_return_results:\n",
    "            probs = probs.tolist()\n",
    "            for prob in probs:\n",
    "                result = []\n",
    "                for c, pred in enumerate(prob):\n",
    "                    if pred > 0.5:\n",
    "                        result.append(label_vocab[c])\n",
    "                results.append(','.join(result))\n",
    "\n",
    "    auc, f1_score, precison, recall = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, auc: %.5f, f1 score: %.5f, precison: %.5f, recall: %.5f\" %\n",
    "          (np.mean(losses), auc, f1_score, precison, recall))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    if if_return_results:\n",
    "        return results\n",
    "    else:\n",
    "        return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 6 # 训练轮次\n",
    "ckpt_dir = \"ernie_ckpt\" # 训练过程中保存模型参数的文件夹\n",
    "\n",
    "global_step = 0  # 迭代次数\n",
    "tic_train = time.time()\n",
    "best_f1_score = 0\n",
    "\n",
    "# 模型训练\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # 计算模型输出、损失函数值、分类概率值、准确率、f1分数\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.sigmoid(logits)\n",
    "        metric.update(probs, labels)\n",
    "        auc, f1_score, _, _ = metric.accumulate()\n",
    "\n",
    "        # 每迭代10次，打印损失函数值、准确率、f1分数、计算速度\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, auc: %.5f, f1 score: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, auc, f1_score,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        \n",
    "        # 每迭代40次，评估当前训练的模型、保存当前最佳模型参数和分词器的词表等\n",
    "        if global_step % 40 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            eval_f1_score = evaluate(model, criterion, metric, test_data_loader, label_vocab, if_return_results=False)\n",
    "            if eval_f1_score > best_f1_score:\n",
    "                best_f1_score = eval_f1_score\n",
    "                model.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T06:00:20.434888Z",
     "iopub.status.busy": "2022-11-16T06:00:20.434302Z",
     "iopub.status.idle": "2022-11-16T06:00:21.170415Z",
     "shell.execute_reply": "2022-11-16T06:00:21.169006Z",
     "shell.execute_reply.started": "2022-11-16T06:00:20.434858Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 由于data目录下文件不保存，故将训练好的模型参数复制到work目录下便于存储\n",
    "!cp /home/aistudio/data/data177486/ernie_ckpt/model_state.pdparams /home/aistudio/work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:18:34.670040Z",
     "iopub.status.busy": "2022-11-16T07:18:34.669391Z",
     "iopub.status.idle": "2022-11-16T07:18:44.293141Z",
     "shell.execute_reply": "2022-11-16T07:18:44.292288Z",
     "shell.execute_reply.started": "2022-11-16T07:18:34.670002Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNIE 3.0 在GoEmotions微情感28分类test集表现： eval loss: 0.08809, auc: 0.94732, f1 score: 0.59687, precison: 0.57566, recall: 0.61969\r\n"
     ]
    }
   ],
   "source": [
    "# 加载训练好的模型最优参数\n",
    "model.set_dict(paddle.load('ernie_ckpt/model_state.pdparams'))\n",
    "\n",
    "# 加载之前训练好的模型参数\n",
    "# model.set_dict(paddle.load('/home/aistudio/work/model_state.pdparams'))\n",
    "\n",
    "# 模型验证\n",
    "print(\"ERNIE 3.0 在GoEmotions微情感28分类test集表现：\", end= \" \")\n",
    "results = evaluate(model, criterion, metric, test_data_loader, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GoEmotions论文提供基线实验结果：**\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/d88c3611c4294462877c3d396a51feb4729ab52e00c54805a65989174781449e)\n",
    "\n",
    "感兴趣的可以基于提供基线做更多对比实验！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8  “微”情感28多标签分类预测演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:19:00.470217Z",
     "iopub.status.busy": "2022-11-16T07:19:00.469784Z",
     "iopub.status.idle": "2022-11-16T07:19:00.481465Z",
     "shell.execute_reply": "2022-11-16T07:19:00.480546Z",
     "shell.execute_reply.started": "2022-11-16T07:19:00.470183Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义数据加载和处理函数\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "def convert_example(example, tokenizer, max_seq_length=64, is_test=False):\n",
    "    qtconcat = example[\"text\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# 定义模型预测函数\n",
    "def predict(model, data, tokenizer, label_vocab, batch_size=1, max_seq=64):\n",
    "    examples = []\n",
    "    # 将输入数据（list格式）处理为模型可接受的格式\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=max_seq,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.sigmoid(logits)\n",
    "        probs = probs.tolist()\n",
    "        # 结果处理,选取概率大于0.5的情感类别\n",
    "        for prob in probs:\n",
    "            result = []\n",
    "            for c, pred in enumerate(prob):\n",
    "                if pred > 0.5:\n",
    "                    result.append(label_vocab[c])\n",
    "            results.append(','.join(result))\n",
    "    return results  # 返回预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T07:19:04.842933Z",
     "iopub.status.busy": "2022-11-16T07:19:04.841883Z",
     "iopub.status.idle": "2022-11-16T07:19:04.894854Z",
     "shell.execute_reply": "2022-11-16T07:19:04.894050Z",
     "shell.execute_reply.started": "2022-11-16T07:19:04.842892Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Thats absolutely disgusting. \t Lables: disgust\r\n",
      "Text: Why would I do that? \t Lables: curiosity\r\n",
      "Text: You shut your mouth \t Lables: anger\r\n",
      "Text: Thank you. \t Lables: gratitude\r\n"
     ]
    }
   ],
   "source": [
    "# 定义要进行微情感分析的文本数据\n",
    "data = [\n",
    "    # 11 disgust\n",
    "    {\"text\": 'Thats absolutely disgusting.'},\n",
    "    # 7 curiosity\n",
    "    {\"text\":'Why would I do that?'},\n",
    "    # 2 anger\n",
    "    {\"text\":\"You shut your mouth\"},\n",
    "    # 15 gratitude\n",
    "    {\"text\":\"Thank you.\"}\n",
    "]\n",
    "\n",
    "# 模型预测\n",
    "labels =  predict(model, data, tokenizer, label_vocab, batch_size=1)\n",
    "\n",
    "# 输出预测结果\n",
    "for idx, text in enumerate(data):\n",
    "    print('Text: {} \\t Lables: {}'.format(text['text'], labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后续将带来该项目的部署演示，期待的小伙伴可以点个喜欢❤关注本项目！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四.项目总结\n",
    "\n",
    "GoEmotions 28微情感多分类数据集首次将情感类别提升至28种，为更好挖掘用户潜在情感提供了契机，具有很大研究意义。精细化的微情感分类以及多标签分类是本次项目遇到的主要挑战。在本次项目中，我们通过预训练模型Erinie3.0在处理后的GoEmotions数据集上进行微调训练，搭建了微情感多分类模型，细化情感分析结果粒度同时优化了微情感分类效果，相较传统2-7情感分类，可以更好挖掘用户潜在情感从而更好发挥微情感分析价值。\n",
    "\n",
    "**项目不足与后续方向：**\n",
    "\n",
    "1.从F1-Score等模型评估指标上看，模型效果还有待提升，后续可以从模型优化、调参（max_seq_length、batch_size、learning_rate、weight_decay等）等多角度进行进一步优化。\n",
    "\n",
    "2.英文数据集导致中文场景受限。针对这一不足，我们可以考虑在中文数据集上进行迁移训练或者通过翻译进行中译英从而泛化使用场景。\n",
    "\n",
    "3.后续会考虑对该项目进行部署应用，搭建微情感分析平台，通过可视化界面更好演示功能。同时结合舆情分析等应用更好发挥微情感分析价值。\n",
    "\n",
    "**参考项目：**  [【快速上手ERNIE 3.0】法律文本多标签分类实战](https://aistudio.baidu.com/aistudio/projectdetail/3996601)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五.作者介绍\n",
    "> **昵称：**[炼丹师233](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/330406)\n",
    "\n",
    "> **飞桨开发者技术专家 PPDE**\n",
    "\n",
    "> **Github主页：**[https://github.com/hchhtc123](https://github.com/hchhtc123)\n",
    "\n",
    "> **研究方向：**   全栈小菜鸡，主攻大数据开发和NLP方向，喜欢捣鼓有趣项目。\n",
    "\n",
    "> [https://aistudio.baidu.com/aistudio/personalcenter/thirdview/330406](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/330406) 关注我，下次带来更多精彩项目分享！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

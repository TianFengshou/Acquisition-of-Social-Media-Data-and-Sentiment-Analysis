{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然语言情感分析和文本匹配是日常生活中最常用的两类自然语言处理任务，本节主要介绍情感分析和文本匹配原理实现和典型模型，以及如何使用飞桨完成情感分析任务。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然语言情感分析\n",
    "\n",
    "人类自然语言具有高度的复杂性，相同的对话在不同的情景，不同的情感，不同的人演绎，表达的效果往往也会迥然不同。例如\"你真的太瘦了\"，当你聊天的对象是一位身材苗条的人，这是一句赞美的话；当你聊天的对象是一位肥胖的人时，这就变成了一句嘲讽。感兴趣的读者可以看一段来自肥伦秀的[视频片段](https://www.bilibili.com/video/av40396494?from=search&seid=9852893210841347755)，继续感受下人类语言情感的复杂性。\n",
    "\n",
    "从视频中的内容可以看出，人类自然语言不只具有复杂性，同时也蕴含着丰富的情感色彩：表达人的情绪（如悲伤、快乐）、表达人的心情（如倦怠、忧郁）、表达人的喜好（如喜欢、讨厌）、表达人的个性特征和表达人的立场等等。利用机器自动分析这些情感倾向，不但有助于帮助企业了解消费者对其产品的感受，为产品改进提供依据；同时还有助于企业分析商业伙伴们的态度，以便更好地进行商业决策。\n",
    "\n",
    "简单的说，我们可以将情感分析（sentiment classification）任务定义为一个分类问题，即指定一个文本输入，机器通过对文本进行分析、处理、归纳和推理后自动输出结论，如**图1**所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/a9a35e01fc7b46b58b13e921207331cc895c7befdc964a0da088acc88479cbc0\" width=\"800\" ></center>\n",
    "<br><center>图1：情感分析任务</center></br>\n",
    "\n",
    "通常情况下，人们把情感分析任务看成一个三分类问题，如 **图2** 所示：\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b630901b397e4e7a8e78ab1d306dfa1fc070d91015a64ef0b8d590aaa8cfde14\" width=\"600\" ></center>\n",
    "<br><center>图2：情感分析任务</center></br>\n",
    "\n",
    "- **正向：** 表示正面积极的情感，如高兴，幸福，惊喜，期待等。\n",
    "- **负向：** 表示负面消极的情感，如难过，伤心，愤怒，惊恐等。\n",
    "- **其他：** 其他类型的情感。\n",
    "\n",
    "在情感分析任务中，研究人员除了分析句子的情感类型外，还细化到以句子中具体的“方面”为分析主体进行情感分析（aspect-level），如下：\n",
    "\n",
    "> 这个薯片口味有点咸，太辣了，不过口感很脆。\n",
    "\n",
    "关于薯片的口味方面是一个负向评价（咸，太辣），然而对于口感方面却是一个正向评价（很脆）。\n",
    "\n",
    "> 我很喜欢夏威夷，就是这边的海鲜太贵了。\n",
    "\n",
    "关于夏威夷是一个正向评价（喜欢），然而对于夏威夷的海鲜却是一个负向评价（价格太贵）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用深度神经网络完成情感分析任务\n",
    "\n",
    "上一节课我们学习了通过把每个单词转换成向量的方式，可以完成单词语义计算任务。那么我们自然会联想到，是否可以把每个自然语言句子也转换成一个向量表示，并使用这个向量表示完成情感分析任务呢？\n",
    "\n",
    "在日常工作中有一个非常简单粗暴的解决方式：就是先把一个句子中所有词的embedding进行加权平均，再用得到的平均embedding作为整个句子的向量表示。然而由于自然语言变幻莫测，我们在使用神经网络处理句子的时候，往往会遇到如下两类问题：\n",
    "\n",
    "- **变长的句子：** 自然语言句子往往是变长的，不同的句子长度可能差别很大。然而大部分神经网络接受的输入都是张量，长度是固定的，那么如何让神经网络处理变长数据成为了一大挑战。\n",
    "\n",
    "- **组合的语义：** 自然语言句子往往对结构非常敏感，有时稍微颠倒单词的顺序都可能改变这句话的意思，比如：\n",
    "\n",
    "  >你等一下我做完作业就走。\n",
    "  >\n",
    "  >我等一下你做完工作就走。\n",
    "\n",
    "  >我不爱吃你做的饭。\n",
    "  >\n",
    "  >你不爱吃我做的饭。\n",
    "  \n",
    "  >我瞅你咋地。\n",
    "  >\n",
    "  >你瞅我咋地。\n",
    "  \n",
    "因此，我们需要找到一个可以考虑词和词之间顺序（关系）的神经网络，用于更好地实现自然语言句子建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理变长数据\n",
    "\n",
    "在使用神经网络处理变长数据时，需要先设置一个全局变量max_seq_len，再对语料中的句子进行处理，将不同的句子组成mini-batch，用于神经网络学习和处理。\n",
    "\n",
    "**1. 设置全局变量**\n",
    "\n",
    "设定一个全局变量max_seq_len，用来控制神经网络最大可以处理文本的长度。我们可以先观察语料中句子的分布，再设置合理的max_seq_len值，以最高的性价比完成句子分类任务（如情感分类）。\n",
    "\n",
    "**2. 对语料中的句子进行处理**\n",
    "\n",
    "我们通常采用 **截断+填充** 的方式，对语料中的句子进行处理，将不同长度的句子组成mini-batch，以便让句子转换成一个张量给神经网络进行计算，如 **图 3** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b539dd187fce48b7b4083f4917d28542de241257331d44efb688ad613281f067\" width=\"400\" ></center>\n",
    "<br><center>图3：变长数据处理</center></br>\n",
    "\n",
    "* 对于长度超过max_seq_len的句子，通常会把这个句子进行截断，以便可以输入到一个张量中。句子截断是有技巧的，有时截取句子的前一部分会比后一部分好，有时则恰好相反。当然也存在其他的截断方式，有兴趣的读者可以翻阅一下相关资料，这里不做赘述。\n",
    "  - **前向截断：** “晚饭， 真， 难， 以， 下， 咽”\n",
    "  - **后向截断：**“今天， 的， 晚饭， 真， 难， 以”\n",
    "* 对于句子长度不足max_seq_len的句子，我们一般会使用一个特殊的词语对这个句子进行填充，这个过程称为Padding。假设给定一个句子“我，爱，人工，智能”，max_seq_len=6，那么可能得到两种填充方式：\n",
    "  - **前向填充：** “[pad]，[pad]，我，爱，人工，智能”\n",
    "  - **后向填充：**“我，爱，人工，智能，[pad]，[pad]”\n",
    "\n",
    "同样，不同的填充方式也对网络训练效果有一定影响。一般来说，后向填充是更常用的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习句子的语义\n",
    "\n",
    "上一节课学习了如何学习每个单词的语义信息，从上面的举例中我们也会观察到，一个句子中词的顺序往往对这个句子的整体语义有重要的影响。因此，在刻画整个句子的语义信息过程中，不能撇开顺序信息。如果简单粗暴地把这个句子中所有词的向量做加和，会使得我们的模型无法区分句子的真实含义，例如：\n",
    "\n",
    ">我不爱吃你做的饭。\n",
    ">\n",
    ">你不爱吃我做的饭。\n",
    "\n",
    "一个有趣的想法，把一个自然语言句子看成一个序列，把自然语言的生成过程看成是一个序列生成的过程。例如对于句子“我，爱，人工，智能”，这句话的生成概率$\\text{P}(\\text{我，爱，人工，智能})$可以被表示为：\n",
    "\n",
    "$\\text{P}(\\text{我，爱，人工，智能})=\\text{P}(我|\\text{<s>})*\\text{P}(爱|\\text{<s>，我})* \\text{P}(人工|\\text{<s>，我，爱})* \\text{P}(智能|\\text{<s>，我，爱，人工})* \\text{P}(\\text{</s>}|\\text{<s>，我，爱，人工，智能})$\n",
    "\n",
    "其中$\\text{<s>}$和$\\text{</s>}$是两个特殊的不可见符号，表示一个句子在逻辑上的开始和结束。\n",
    "\n",
    "上面的公式把一个句子的生成过程建模成一个序列的决策过程，这就是香农在1950年左右提出的使用马尔可夫过程建模自然语言的思想。使用序列的视角看待和建模自然语言有一个明显的好处，那就是在对每个词建模的过程中，都有机会去学习这个词和之前生成的词之间的关系，并利用这种关系更好地处理自然语言。如 **图4** 所示，生成句子“我，爱，人工”后，“智能”在下一步生成的概率就变得很高了，因为“人工智能”经常同时出现。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b8f6029b46fa4ecca92e369952e4404dd93c36be1faf4b7ba603ac7b35fb6e4b\" width=\"600\" ></center>\n",
    "<br><center>图4：自然语言生成过程示意图</center></br>\n",
    "\n",
    "通过考虑句子内部的序列关系，我们就可以清晰地区分“我不爱吃你做的菜”和“你不爱吃我做的菜”这两句话之间的联系与不同了。事实上，目前大多数成功的自然语言模型都建立在对句子的序列化建模上。下面让我们学习两个经典的序列化建模模型：循环神经网络（Recurrent Neural Network，RNN）和长短时记忆网络（Long Short-Term Memory，LSTM）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络RNN和长短时记忆网络LSTM\n",
    "\n",
    "## RNN和LSTM网络的设计思考\n",
    "\n",
    "与读者熟悉的卷积神经网络（Convolutional Neural Networks, CNN）一样，各种形态的神经网络在设计之初，均有针对特定场景的奇思妙想。卷积神经网络的设计具备适合视觉任务“局部视野”特点，是因为视觉信息是局部有效的。例如在一张图片的1/4区域上有一只小猫，如果将图片3/4的内容遮挡，人类依然可以判断这是一只猫。\n",
    "\n",
    "与此类似，RNN和LSTM的设计初衷是部分场景神经网络需要有“记忆”能力才能解决的任务。在自然语言处理任务中，往往一段文字中某个词的语义可能与前一段句子的语义相关，只有记住了上下文的神经网络才能很好的处理句子的语义关系。例如：\n",
    "> 我一边吃着苹果，一边玩着苹果手机。\n",
    "\n",
    "网络只有正确的记忆两个“苹果”的上下文“吃着”和“玩着…手机”，才能正确的识别两个苹果的语义，分别是水果和手机品牌。如果网络没有记忆功能，那么两个“苹果”只能归结到更高概率出现的语义上，得到一个相同的语义输出，这显然是不合理的。\n",
    "\n",
    "如何设计神经网络的记忆功能呢？我们先了解下RNN网络是如何实现具备记忆功能的。RNN相当于将神经网络单元进行了横向连接，处理前一部分输入的RNN单元不仅有正常的模型输出，还会输出“记忆”传递到下一个RNN单元。而处于后一部分的RNN单元，不仅仅有来自于任务数据的输入，同时会接收从前一个RNN单元传递过来的记忆输入，这样就使得整个神经网络具备了“记忆”能力。\n",
    "\n",
    "但是RNN网络只是初步实现了“记忆”功能，在此基础上科学家们又发明了一些RNN的变体，来加强网络的记忆能力。但RNN对“记忆”能力的设计是比较粗糙的，当网络处理的序列数据过长时，累积的内部信息就会越来越复杂，直到超过网络的承载能力，通俗的说“事无巨细的记录，总有一天大脑会崩溃”。为了解决这个问题，科学家巧妙的设计了一种记忆单元，称之为“长短时记忆网络（Long Short-Term Memory，LSTM）”。在每个处理单元内部，加入了输入门、输出门和遗忘门的设计，三者有明确的任务分工：\n",
    "\n",
    "* 输入门：控制有多少输入信号会被融合；\n",
    "* 遗忘门：控制有多少过去的记忆会被遗忘；\n",
    "* 输出门：控制多少处理后的信息会被输出；\n",
    "\n",
    "三者的作用与人类的记忆方式有异曲同工之处，即：\n",
    "\n",
    "* 与当前任务无关的信息会直接过滤掉，如非常专注的开车时，人们几乎不注意沿途的风景；\n",
    "* 过去记录的事情不一定都要永远记住，如令人伤心或者不重要的事，通常会很快被淡忘；\n",
    "* 根据记忆和现实观察进行决策，如开车时会结合记忆中的路线和当前看到的路标，决策转弯或不做任何动作。\n",
    "\n",
    "了解了这些关于网络设计的本质，下面进入实现方案的细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN网络结构\n",
    "\n",
    "RNN是一个非常经典的面向序列的模型，可以对自然语言句子或是其它时序信号进行建模，网络结构如 **图5** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3f4d413393824d208177a020dcfa68205fa192ebecf04b42a5a17cbe7a146abb\" width=\"600\" ></center>\n",
    "<br><center>图5：RNN网络结构</center></br>\n",
    "\n",
    "不同于其他常见的神经网络结构，循环神经网络的输入是一个序列信息。假设给定任意一句话$[x_0, x_1, ..., x_n]$，其中每个$x_i$都代表了一个词，如“我，爱，人工，智能”。循环神经网络从左到右逐词阅读这个句子，并不断调用一个相同的RNN Cell来处理时序信息。每阅读一个单词，循环神经网络会先将本次输入的单词通过embedding lookup转换为一个向量表示。再把这个单词的向量表示和这个模型内部记忆的向量$h$融合起来，形成一个更新的记忆。最后将这个融合后的表示输出出来，作为它当前阅读到的所有内容的语义表示。当循环神经网络阅读过整个句子之后，我们就可以认为它的最后一个输出状态表示了整个句子的语义信息。\n",
    "\n",
    "听上去很复杂，下面我们以一个简单地例子来说明，假设输入的句子为：\n",
    "\n",
    "> “我，爱，人工，智能”\n",
    "\n",
    "循环神经网络开始从左到右阅读这个句子，在未经过任何阅读之前，循环神经网络中的记忆向量是空白的。其处理逻辑如下：\n",
    "1. 网络阅读单词“我”，并把单词“我”的向量表示和空白记忆相融合，输出一个向量$h_1$，用于表示“空白+我”的语义。\n",
    "1. 网络开始阅读单词“爱”，这时循环神经网络内部存在“空白+我”的记忆。循环神经网络会将“空白+我”和“爱”的向量表示相融合，并输出“空白+我+爱”的向量表示$h_2$，用于表示“我爱”这个短语的语义信息。\n",
    "1. 网络开始阅读单词“人工”，同样经过融合之后，输出“空白+我+爱+人工”的向量表示$h_3$，用于表示“空白+我+爱+人工”语义信息。\n",
    "1. 最终在网络阅读了“智能”单词后，便可以输出“我爱人工智能”这一句子的整体语义信息。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "在实现当前输入$x_t$和已有记忆$h_{t-1}$融合的时候，循环神经网络采用相加并通过一个激活函数tanh的方式实现：\n",
    "\n",
    "$h_t = tanh(WX_{t}+VH_{t-1}+b)$\n",
    "\n",
    "tanh函数是一个值域为（-1,1）的函数，其作用是长期维持内部记忆在一个固定的数值范围内，防止因多次迭代更新导致数值爆炸。同时tanh的导数是一个平滑的函数，会让神经网络的训练变得更加简单。\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM网络结构\n",
    "\n",
    "上述方法听上去很有效（事实上在有些任务上效果还不错），但是存在一个明显的缺陷，就是当阅读很长的序列时，网络内部的信息会变得越来越复杂，甚至会超过网络的记忆能力，使得最终的输出信息变得混乱无用。长短时记忆网络（Long Short-Term Memory，LSTM）内部的复杂结构正是为处理这类问题而设计的，其网络结构如 **图6** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/d7d112d458ff401ba7fb089cf0521b284c795798dcfb4b88bd2dd6b65854193a\" width=\"600\" ></center>\n",
    "<br><center>图6：LSTM网络结构</center></br>\n",
    "\n",
    "长短时记忆网络的结构和循环神经网络非常类似，都是通过不断调用同一个cell来逐次处理时序信息。每阅读一个新单词$x_t$，就会输出一个新的输出信号$h_t$，用来表示当前阅读到所有内容的整体向量表示。不过二者又有一个明显区别，长短时记忆网络在不同cell之间传递的是两个记忆信息，而不像循环神经网络一样只有一个记忆信息，此外长短时记忆网络的内部结构也更加复杂，如 **图7** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/024b611db4ad4b709a4e1bd403ac41897c73792b895d4557a2dd561d625fcbf1\" width=\"400\" ></center>\n",
    "<br><center>图7：LSTM网络内部结构示意图</center></br>\n",
    "\n",
    "区别于循环神经网络RNN，长短时记忆网络最大的特点是在更新内部记忆时，引入了遗忘机制。即容许网络忘记过去阅读过程中看到的一些无关紧要的信息，只保留有用的历史信息。通过这种方式延长了记忆长度。举个例子：\n",
    "\n",
    "> 我觉得这家餐馆的菜品很不错，烤鸭非常正宗，包子也不错，酱牛肉很有嚼劲。但是服务员态度太恶劣了，我们在门口等了50分钟都没有能成功进去，好不容易进去了，桌子也半天没人打扫。整个环境非常吵闹，我的孩子都被吓哭了，我下次不会带朋友来。\n",
    "\n",
    "当我们阅读上面这段话的时候，可能会记住一些关键词，如烤鸭好吃、牛肉有嚼劲、环境吵等，但也会忽略一些不重要的内容，如“我觉得”、“好不容易”等，长短时记忆网络正是受这个启发而设计的。\n",
    "\n",
    "长短时记忆网络的Cell有三个输入：\n",
    "\n",
    "- 这个网络新看到的输入信号，如下一个单词，记为$x_{t}$， 其中$x_{t}$是一个向量，$t$代表了当前时刻。\n",
    "- 这个网络在上一步的输出信号，记为$h_{t-1}$，这是一个向量，维度同$x_{t}$相同。\n",
    "- 这个网络在上一步的记忆信号，记为$c_{t-1}$，这是一个向量，维度同$x_{t}$相同。\n",
    "\n",
    "得到这两个信号之后，长短时记忆网络没有立即去融合这两个向量，而是计算了权重。\n",
    "\n",
    "- 输入门：$i_{t}=sigmoid(W_{i}X_{t}+V_{i}H_{t-1}+b_i)$，控制有多少输入信号会被融合。\n",
    "- 遗忘门：$f_{t}=sigmoid(W_{f}X_{t}+V_{f}H_{t-1}+b_f)$，控制有多少过去的记忆会被遗忘。\n",
    "- 输出门：$o_{t}=sigmoid(W_{o}X_{t}+V_{o}H_{t-1}+b_o)$，控制最终输出多少融合了记忆的信息。\n",
    "- 单元状态：$g_{t}=tanh(W_{g}X_{t}+V_{g}H_{t-1}+b_g)$，输入信号和过去的输入信号做一个信息融合。\n",
    "\n",
    "通过学习这些门的权重设置，长短时记忆网络可以根据当前的输入信号和记忆信息，有选择性地忽略或者强化当前的记忆或是输入信号，帮助网络更好地学习长句子的语义信息：\n",
    "\n",
    "- 记忆信号：$c_{t} = f_{t} \\cdot c_{t-1} + i_{t} \\cdot g_{t}$\n",
    "\n",
    "- 输出信号：$h_{t} = o_{t} \\cdot tanh(c_{t})$\n",
    "\n",
    "------\n",
    "\n",
    "**说明：**\n",
    "\n",
    "事实上，长短时记忆网络之所以能更好地对长文本进行建模，还存在另外一套更加严谨的计算和证明，有兴趣的读者可以翻阅一下引文中的参考资料进行详细研究。\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用LSTM完成情感分析任务\n",
    "\n",
    "借助长短时记忆网络，我们可以非常轻松地完成情感分析任务。如 **图8** 所示。对于每个句子，我们首先通过截断和填充的方式，把这些句子变成固定长度的向量。然后，利用长短时记忆网络，从左到右开始阅读每个句子。在完成阅读之后，我们使用长短时记忆网络的最后一个输出记忆，作为整个句子的语义信息，并直接把这个向量作为输入，送入一个分类层进行分类，从而完成对情感分析问题的神经网络建模。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/45a12932d1be43b58e4daa1e2c3f6c34363333c764064366986b86915beb4e04\" width=\"400\" ></center>\n",
    "<br><center>图8：LSTM完成情感分析任务流程</center></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用飞桨实现基于LSTM的情感分析模型 \n",
    "\n",
    "接下来让我们看看如何使用飞桨实现一个基于长短时记忆网络的情感分析模型。在飞桨中，不同深度学习模型的训练过程基本一致，流程如下：\n",
    "\n",
    "1. **数据处理**：选择需要使用的数据，并做好必要的预处理工作。\n",
    "\n",
    "2. **网络定义**：使用飞桨定义好网络结构，包括输入层，中间层，输出层，损失函数和优化算法。\n",
    "\n",
    "3. **网络训练**：将准备好的数据送入神经网络进行学习，并观察学习的过程是否正常，如损失函数值是否在降低，也可以打印一些中间步骤的结果出来等。\n",
    "\n",
    "4. **网络评估**：使用测试集合测试训练好的神经网络，看看训练效果如何。\n",
    "\n",
    "在数据处理前，需要先加载飞桨平台（如果用户在本地使用，请确保已经安装飞桨）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import re\n",
    "import random\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.nn import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "首先，需要下载语料用于模型训练和评估效果。我们使用的是IMDB的电影评论数据，这个数据集是一个开源的英文数据集，由训练数据和测试数据组成。每个数据都分别由若干小文件组成，每个小文件内部都是一段用户关于某个电影的真实评价，以及观众对这个电影的情感倾向（是正向还是负向），数据集下载的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download():\n",
    "    # 通过python的requests类，下载存储在\n",
    "    # https://dataset.bj.bcebos.com/imdb%2FaclImdb_v1.tar.gz的文件\n",
    "    corpus_url = \"https://dataset.bj.bcebos.com/imdb%2FaclImdb_v1.tar.gz\"\n",
    "    web_request = requests.get(corpus_url)\n",
    "    corpus = web_request.content\n",
    "\n",
    "    # 将下载的文件写在当前目录的aclImdb_v1.tar.gz文件内\n",
    "    with open(\"./aclImdb_v1.tar.gz\", \"wb\") as f:\n",
    "        f.write(corpus)\n",
    "    f.close()\n",
    "\n",
    "download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，将数据集加载到程序中，并打印一小部分数据观察一下数据集的特点，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:14: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:13: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:14: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:13: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:14: DeprecationWarning: invalid escape sequence \\.\n",
      "<ipython-input-3-84f11aff8edb>:13: DeprecationWarning: invalid escape sequence \\.\n",
      "  path_pattern = \"aclImdb/train/\" + label + \"/.*\\.txt$\" if is_training \\\n",
      "<ipython-input-3-84f11aff8edb>:14: DeprecationWarning: invalid escape sequence \\.\n",
      "  else \"aclImdb/test/\" + label + \"/.*\\.txt$\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0, Zentropa has much in common with The Third Man, another noir-like film set among the rubble of postwar Europe. Like TTM, there is much inventive camera work. There is an innocent American who gets emotionally involved with a woman he doesn't really understand, and whose naivety is all the more striking in contrast with the natives.<br /><br />But I'd have to say that The Third Man has a more well-crafted storyline. Zentropa is a bit disjointed in this respect. Perhaps this is intentional: it is presented as a dream/nightmare, and making it too coherent would spoil the effect. <br /><br />This movie is unrelentingly grim--\"noir\" in more than one sense; one never sees the sun shine. Grim, but intriguing, and frightening.\n",
      "sentence 0, label 1\n",
      "sentence 1, Zentropa is the most original movie I've seen in years. If you like unique thrillers that are influenced by film noir, then this is just the right cure for all of those Hollywood summer blockbusters clogging the theaters these days. Von Trier's follow-ups like Breaking the Waves have gotten more acclaim, but this is really his best work. It is flashy without being distracting and offers the perfect combination of suspense and dark humor. It's too bad he decided handheld cameras were the wave of the future. It's hard to say who talked him away from the style he exhibits here, but it's everyone's loss that he went into his heavily theoretical dogma direction instead.\n",
      "sentence 1, label 1\n",
      "sentence 2, Lars Von Trier is never backward in trying out new techniques. Some of them are very original while others are best forgotten.<br /><br />He depicts postwar Germany as a nightmarish train journey. With so many cities lying in ruins, Leo Kessler a young American of German descent feels obliged to help in their restoration. It is not a simple task as he quickly finds out.<br /><br />His uncle finds him a job as a night conductor on the Zentropa Railway Line. His job is to attend to the needs of the passengers. When the shoes are polished a chalk mark is made on the soles. A terrible argument ensues when a passenger's shoes are not chalked despite the fact they have been polished. There are many allusions to the German fanaticism of adherence to such stupid details.<br /><br />The railway journey is like an allegory representing man's procession through life with all its trials and tribulations. In one sequence Leo dashes through the back carriages to discover them filled with half-starved bodies appearing to have just escaped from Auschwitz . These images, horrible as they are, are fleeting as in a dream, each with its own terrible impact yet unconnected.<br /><br />At a station called Urmitz Leo jumps from the train with a parceled bomb. In view of many by-standers he connects the bomb to the underside of a carriage. He returns to his cabin and makes a connection to a time clock. Later he jumps from the train (at high speed) and lies in the cool grass on a river bank. Looking at the stars above he decides that his job is to build and not destroy. Subsequently as he sees the train approaching a giant bridge he runs at breakneck speed to board the train and stop the clock. If you care to analyse the situation it is a completely impossible task. Quite ridiculous in fact. It could only happen in a dream.<br /><br />It's strange how one remembers little details such as a row of cups hanging on hooks and rattling away with the swaying of the train.<br /><br />Despite the fact that this film is widely acclaimed, I prefer Lars Von Trier's later films (Breaking the Waves and The Idiots). The bomb scene described above really put me off. Perhaps I'm a realist.\n",
      "sentence 2, label 1\n",
      "sentence 3, *Contains spoilers due to me having to describe some film techniques, so read at your own risk!*<br /><br />I loved this film. The use of tinting in some of the scenes makes it seem like an old photograph come to life. I also enjoyed the projection of people on a back screen. For instance, in one scene, Leopold calls his wife and she is projected behind him rather than in a typical split screen. Her face is huge in the back and Leo's is in the foreground.<br /><br />One of the best uses of this is when the young boys kill the Ravensteins on the train, a scene shot in an almost political poster style, with facial close ups. It reminded me of Battleship Potemkin, that intense constant style coupled with the spray of red to convey tons of horror without much gore. Same with the scene when Katharina finds her father dead in the bathtub...you can only see the red water on the side. It is one of the things I love about Von Trier, his understatement of horror, which ends up making it all the more creepy.<br /><br />The use of text in the film was unique, like when Leo's character is pushed by the word, \"Werewolf.\" I have never seen anything like that in a film.<br /><br />The use of black comedy in this film was well done. Ernst-Hugo Järegård is great as Leo's uncle. It brings up the snickers I got from his role in the Kingdom (Riget.) This humor makes the plotline of absurd anal retentiveness of train conductors against the terrible backdrop of WW2 and all the chaos, easier to take. It reminds me of Riget in the way the hospital administrator is trying to maintain a normalcy at the end of part one when everything is going crazy. It shows that some people are truly oblivious to the awful things happening around them. Yet some people, like Leo, are tuned in, but do nothing positive about it.<br /><br />The voice over, done expertly well by Max von Sydow, is amusing too. It draws you into the story and makes you jump into Leo's head, which at times is a scary place to be.<br /><br />The movie brings up the point that one is a coward if they don't choose a side. I see the same idea used in Dancer in the Dark, where Bjork's character doesn't speak up for herself and ends up being her own destruction. Actually, at one time, Von Trier seemed anti-woman to me, by making Breaking the Waves and Dancer, but now I know his male characters don't fare well either! I found myself at the same place during the end of Dancer, when you seriously want the main character to rethink their actions, but of course, they never do!\n",
      "sentence 3, label 1\n",
      "sentence 4, That was the first thing that sprang to mind as I watched the closing credits to Europa make there was across the screen, never in my entire life have I seen a film of such technical genius, the visuals of Europa are so impressive that any film I watch in it's wake will only pale in comparison, forget your Michael Bay, Ridley Scott slick Hollywood cinematography, Europa has more ethereal beauty than anything those two could conjure up in a million years. Now I'd be the first to hail Lars von Trier a genius just off the back of his films Breaking the Waves and Dancer in the Dark, but this is stupid, the fact that Europa has gone un-noticed by film experts for so long is a crime against cinema, whilst overrated rubbish like Crouching Tiger, Hidden Dragon and Life is Beautiful clean up at the academy awards (but what do the know) Europa has been hidden away, absent form video stores and (until recently) any British TV channels. <br /><br />The visuals in Europa are not MTV gloss; it's not a case of style over substance, its more a case of substance dictating style. Much like his first film The Element of Crime, von Trier uses the perspective of the main character to draw us into his world, and much like Element, the film begins with the main character (or in the case of Europa, we the audience) being hypnotized. As we move down the tracks, the voice of the Narrator (Max von Sydow) counts us down into a deep sleep, until we awake in Europa. This allows von Trier and his three cinematographers to pay with the conventions of time and imagery, there are many scenes in Europa when a character in the background, who is in black and white, will interact with a person in the foreground who will be colour, von Trier is trying to show us how much precedence the coloured item or person has over the plot, for instance, it's no surprise that the first shot of Leopold Kessler (Jean-marc Barr) is in colour, since he is the only character who's actions have superiority over the film. <br /><br />The performances are good, they may not be on par with performances in later von Trier films, but that's just because the images are sometimes so distracting that you don't really pick up on them the first time round. But I would like to point out the fantastic performance of Jean-Marc Barr in the lead role, whose blind idealism is slowly warn down by the two opposing sides, until he erupts in the films final act. Again, muck like The Element of Crime, the film ends with our hero unable to wake up from his nightmare state, left in this terrible place, with only the continuing narration of von Sydow to seal his fate. Europa is a tremendous film, and I cant help thinking what a shame that von Trier has abandoned this way of filming, since he was clearly one of the most talented visual directors working at that time, Europa, much like the rest of his cinematic cannon is filled with a wealth of iconic scenes. His dedication to composition and mise-en-scene is unrivalled, not to mention his use of sound and production design. But since his no-frills melodramas turned out to be Breaking the Waves and Dancer in the Dark then who can argue, but it does seems like a waste of an imaginative talent. 10/10\n",
      "sentence 4, label 1\n"
     ]
    }
   ],
   "source": [
    "def load_imdb(is_training):\n",
    "    data_set = []\n",
    "\n",
    "    # aclImdb_v1.tar.gz解压后是一个目录\n",
    "    # 我们可以使用python的rarfile库进行解压\n",
    "    # 训练数据和测试数据已经经过切分，其中训练数据的地址为：\n",
    "    # ./aclImdb/train/pos/ 和 ./aclImdb/train/neg/，分别存储着正向情感的数据和负向情感的数据\n",
    "    # 我们把数据依次读取出来，并放到data_set里\n",
    "    # data_set中每个元素都是一个二元组，（句子，label），其中label=0表示负向情感，label=1表示正向情感\n",
    "    \n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        with tarfile.open(\"./aclImdb_v1.tar.gz\") as tarf:\n",
    "            path_pattern = \"aclImdb/train/\" + label + \"/.*\\.txt$\" if is_training \\\n",
    "                else \"aclImdb/test/\" + label + \"/.*\\.txt$\"\n",
    "            path_pattern = re.compile(path_pattern)\n",
    "            tf = tarf.next()\n",
    "            while tf != None:\n",
    "                if bool(path_pattern.match(tf.name)):\n",
    "                    sentence = tarf.extractfile(tf).read().decode()\n",
    "                    sentence_label = 0 if label == 'neg' else 1\n",
    "                    data_set.append((sentence, sentence_label)) \n",
    "                tf = tarf.next()\n",
    "\n",
    "    return data_set\n",
    "\n",
    "train_corpus = load_imdb(True)\n",
    "test_corpus = load_imdb(False)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"sentence %d, %s\" % (i, train_corpus[i][0]))    \n",
    "    print(\"sentence %d, label %d\" % (i, train_corpus[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，在自然语言处理中，需要先对语料进行切词，这里我们可以使用空格把每个句子切成若干词的序列，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['zentropa', 'has', 'much', 'in', 'common', 'with', 'the', 'third', 'man,', 'another', 'noir-like', 'film', 'set', 'among', 'the', 'rubble', 'of', 'postwar', 'europe.', 'like', 'ttm,', 'there', 'is', 'much', 'inventive', 'camera', 'work.', 'there', 'is', 'an', 'innocent', 'american', 'who', 'gets', 'emotionally', 'involved', 'with', 'a', 'woman', 'he', \"doesn't\", 'really', 'understand,', 'and', 'whose', 'naivety', 'is', 'all', 'the', 'more', 'striking', 'in', 'contrast', 'with', 'the', 'natives.<br', '/><br', '/>but', \"i'd\", 'have', 'to', 'say', 'that', 'the', 'third', 'man', 'has', 'a', 'more', 'well-crafted', 'storyline.', 'zentropa', 'is', 'a', 'bit', 'disjointed', 'in', 'this', 'respect.', 'perhaps', 'this', 'is', 'intentional:', 'it', 'is', 'presented', 'as', 'a', 'dream/nightmare,', 'and', 'making', 'it', 'too', 'coherent', 'would', 'spoil', 'the', 'effect.', '<br', '/><br', '/>this', 'movie', 'is', 'unrelentingly', 'grim--\"noir\"', 'in', 'more', 'than', 'one', 'sense;', 'one', 'never', 'sees', 'the', 'sun', 'shine.', 'grim,', 'but', 'intriguing,', 'and', 'frightening.'], 1), (['zentropa', 'is', 'the', 'most', 'original', 'movie', \"i've\", 'seen', 'in', 'years.', 'if', 'you', 'like', 'unique', 'thrillers', 'that', 'are', 'influenced', 'by', 'film', 'noir,', 'then', 'this', 'is', 'just', 'the', 'right', 'cure', 'for', 'all', 'of', 'those', 'hollywood', 'summer', 'blockbusters', 'clogging', 'the', 'theaters', 'these', 'days.', 'von', \"trier's\", 'follow-ups', 'like', 'breaking', 'the', 'waves', 'have', 'gotten', 'more', 'acclaim,', 'but', 'this', 'is', 'really', 'his', 'best', 'work.', 'it', 'is', 'flashy', 'without', 'being', 'distracting', 'and', 'offers', 'the', 'perfect', 'combination', 'of', 'suspense', 'and', 'dark', 'humor.', \"it's\", 'too', 'bad', 'he', 'decided', 'handheld', 'cameras', 'were', 'the', 'wave', 'of', 'the', 'future.', \"it's\", 'hard', 'to', 'say', 'who', 'talked', 'him', 'away', 'from', 'the', 'style', 'he', 'exhibits', 'here,', 'but', \"it's\", \"everyone's\", 'loss', 'that', 'he', 'went', 'into', 'his', 'heavily', 'theoretical', 'dogma', 'direction', 'instead.'], 1), (['lars', 'von', 'trier', 'is', 'never', 'backward', 'in', 'trying', 'out', 'new', 'techniques.', 'some', 'of', 'them', 'are', 'very', 'original', 'while', 'others', 'are', 'best', 'forgotten.<br', '/><br', '/>he', 'depicts', 'postwar', 'germany', 'as', 'a', 'nightmarish', 'train', 'journey.', 'with', 'so', 'many', 'cities', 'lying', 'in', 'ruins,', 'leo', 'kessler', 'a', 'young', 'american', 'of', 'german', 'descent', 'feels', 'obliged', 'to', 'help', 'in', 'their', 'restoration.', 'it', 'is', 'not', 'a', 'simple', 'task', 'as', 'he', 'quickly', 'finds', 'out.<br', '/><br', '/>his', 'uncle', 'finds', 'him', 'a', 'job', 'as', 'a', 'night', 'conductor', 'on', 'the', 'zentropa', 'railway', 'line.', 'his', 'job', 'is', 'to', 'attend', 'to', 'the', 'needs', 'of', 'the', 'passengers.', 'when', 'the', 'shoes', 'are', 'polished', 'a', 'chalk', 'mark', 'is', 'made', 'on', 'the', 'soles.', 'a', 'terrible', 'argument', 'ensues', 'when', 'a', \"passenger's\", 'shoes', 'are', 'not', 'chalked', 'despite', 'the', 'fact', 'they', 'have', 'been', 'polished.', 'there', 'are', 'many', 'allusions', 'to', 'the', 'german', 'fanaticism', 'of', 'adherence', 'to', 'such', 'stupid', 'details.<br', '/><br', '/>the', 'railway', 'journey', 'is', 'like', 'an', 'allegory', 'representing', \"man's\", 'procession', 'through', 'life', 'with', 'all', 'its', 'trials', 'and', 'tribulations.', 'in', 'one', 'sequence', 'leo', 'dashes', 'through', 'the', 'back', 'carriages', 'to', 'discover', 'them', 'filled', 'with', 'half-starved', 'bodies', 'appearing', 'to', 'have', 'just', 'escaped', 'from', 'auschwitz', '.', 'these', 'images,', 'horrible', 'as', 'they', 'are,', 'are', 'fleeting', 'as', 'in', 'a', 'dream,', 'each', 'with', 'its', 'own', 'terrible', 'impact', 'yet', 'unconnected.<br', '/><br', '/>at', 'a', 'station', 'called', 'urmitz', 'leo', 'jumps', 'from', 'the', 'train', 'with', 'a', 'parceled', 'bomb.', 'in', 'view', 'of', 'many', 'by-standers', 'he', 'connects', 'the', 'bomb', 'to', 'the', 'underside', 'of', 'a', 'carriage.', 'he', 'returns', 'to', 'his', 'cabin', 'and', 'makes', 'a', 'connection', 'to', 'a', 'time', 'clock.', 'later', 'he', 'jumps', 'from', 'the', 'train', '(at', 'high', 'speed)', 'and', 'lies', 'in', 'the', 'cool', 'grass', 'on', 'a', 'river', 'bank.', 'looking', 'at', 'the', 'stars', 'above', 'he', 'decides', 'that', 'his', 'job', 'is', 'to', 'build', 'and', 'not', 'destroy.', 'subsequently', 'as', 'he', 'sees', 'the', 'train', 'approaching', 'a', 'giant', 'bridge', 'he', 'runs', 'at', 'breakneck', 'speed', 'to', 'board', 'the', 'train', 'and', 'stop', 'the', 'clock.', 'if', 'you', 'care', 'to', 'analyse', 'the', 'situation', 'it', 'is', 'a', 'completely', 'impossible', 'task.', 'quite', 'ridiculous', 'in', 'fact.', 'it', 'could', 'only', 'happen', 'in', 'a', 'dream.<br', '/><br', \"/>it's\", 'strange', 'how', 'one', 'remembers', 'little', 'details', 'such', 'as', 'a', 'row', 'of', 'cups', 'hanging', 'on', 'hooks', 'and', 'rattling', 'away', 'with', 'the', 'swaying', 'of', 'the', 'train.<br', '/><br', '/>despite', 'the', 'fact', 'that', 'this', 'film', 'is', 'widely', 'acclaimed,', 'i', 'prefer', 'lars', 'von', \"trier's\", 'later', 'films', '(breaking', 'the', 'waves', 'and', 'the', 'idiots).', 'the', 'bomb', 'scene', 'described', 'above', 'really', 'put', 'me', 'off.', 'perhaps', \"i'm\", 'a', 'realist.'], 1), (['*contains', 'spoilers', 'due', 'to', 'me', 'having', 'to', 'describe', 'some', 'film', 'techniques,', 'so', 'read', 'at', 'your', 'own', 'risk!*<br', '/><br', '/>i', 'loved', 'this', 'film.', 'the', 'use', 'of', 'tinting', 'in', 'some', 'of', 'the', 'scenes', 'makes', 'it', 'seem', 'like', 'an', 'old', 'photograph', 'come', 'to', 'life.', 'i', 'also', 'enjoyed', 'the', 'projection', 'of', 'people', 'on', 'a', 'back', 'screen.', 'for', 'instance,', 'in', 'one', 'scene,', 'leopold', 'calls', 'his', 'wife', 'and', 'she', 'is', 'projected', 'behind', 'him', 'rather', 'than', 'in', 'a', 'typical', 'split', 'screen.', 'her', 'face', 'is', 'huge', 'in', 'the', 'back', 'and', \"leo's\", 'is', 'in', 'the', 'foreground.<br', '/><br', '/>one', 'of', 'the', 'best', 'uses', 'of', 'this', 'is', 'when', 'the', 'young', 'boys', 'kill', 'the', 'ravensteins', 'on', 'the', 'train,', 'a', 'scene', 'shot', 'in', 'an', 'almost', 'political', 'poster', 'style,', 'with', 'facial', 'close', 'ups.', 'it', 'reminded', 'me', 'of', 'battleship', 'potemkin,', 'that', 'intense', 'constant', 'style', 'coupled', 'with', 'the', 'spray', 'of', 'red', 'to', 'convey', 'tons', 'of', 'horror', 'without', 'much', 'gore.', 'same', 'with', 'the', 'scene', 'when', 'katharina', 'finds', 'her', 'father', 'dead', 'in', 'the', 'bathtub...you', 'can', 'only', 'see', 'the', 'red', 'water', 'on', 'the', 'side.', 'it', 'is', 'one', 'of', 'the', 'things', 'i', 'love', 'about', 'von', 'trier,', 'his', 'understatement', 'of', 'horror,', 'which', 'ends', 'up', 'making', 'it', 'all', 'the', 'more', 'creepy.<br', '/><br', '/>the', 'use', 'of', 'text', 'in', 'the', 'film', 'was', 'unique,', 'like', 'when', \"leo's\", 'character', 'is', 'pushed', 'by', 'the', 'word,', '\"werewolf.\"', 'i', 'have', 'never', 'seen', 'anything', 'like', 'that', 'in', 'a', 'film.<br', '/><br', '/>the', 'use', 'of', 'black', 'comedy', 'in', 'this', 'film', 'was', 'well', 'done.', 'ernst-hugo', 'järegård', 'is', 'great', 'as', \"leo's\", 'uncle.', 'it', 'brings', 'up', 'the', 'snickers', 'i', 'got', 'from', 'his', 'role', 'in', 'the', 'kingdom', '(riget.)', 'this', 'humor', 'makes', 'the', 'plotline', 'of', 'absurd', 'anal', 'retentiveness', 'of', 'train', 'conductors', 'against', 'the', 'terrible', 'backdrop', 'of', 'ww2', 'and', 'all', 'the', 'chaos,', 'easier', 'to', 'take.', 'it', 'reminds', 'me', 'of', 'riget', 'in', 'the', 'way', 'the', 'hospital', 'administrator', 'is', 'trying', 'to', 'maintain', 'a', 'normalcy', 'at', 'the', 'end', 'of', 'part', 'one', 'when', 'everything', 'is', 'going', 'crazy.', 'it', 'shows', 'that', 'some', 'people', 'are', 'truly', 'oblivious', 'to', 'the', 'awful', 'things', 'happening', 'around', 'them.', 'yet', 'some', 'people,', 'like', 'leo,', 'are', 'tuned', 'in,', 'but', 'do', 'nothing', 'positive', 'about', 'it.<br', '/><br', '/>the', 'voice', 'over,', 'done', 'expertly', 'well', 'by', 'max', 'von', 'sydow,', 'is', 'amusing', 'too.', 'it', 'draws', 'you', 'into', 'the', 'story', 'and', 'makes', 'you', 'jump', 'into', \"leo's\", 'head,', 'which', 'at', 'times', 'is', 'a', 'scary', 'place', 'to', 'be.<br', '/><br', '/>the', 'movie', 'brings', 'up', 'the', 'point', 'that', 'one', 'is', 'a', 'coward', 'if', 'they', \"don't\", 'choose', 'a', 'side.', 'i', 'see', 'the', 'same', 'idea', 'used', 'in', 'dancer', 'in', 'the', 'dark,', 'where', \"bjork's\", 'character', \"doesn't\", 'speak', 'up', 'for', 'herself', 'and', 'ends', 'up', 'being', 'her', 'own', 'destruction.', 'actually,', 'at', 'one', 'time,', 'von', 'trier', 'seemed', 'anti-woman', 'to', 'me,', 'by', 'making', 'breaking', 'the', 'waves', 'and', 'dancer,', 'but', 'now', 'i', 'know', 'his', 'male', 'characters', \"don't\", 'fare', 'well', 'either!', 'i', 'found', 'myself', 'at', 'the', 'same', 'place', 'during', 'the', 'end', 'of', 'dancer,', 'when', 'you', 'seriously', 'want', 'the', 'main', 'character', 'to', 'rethink', 'their', 'actions,', 'but', 'of', 'course,', 'they', 'never', 'do!'], 1), (['that', 'was', 'the', 'first', 'thing', 'that', 'sprang', 'to', 'mind', 'as', 'i', 'watched', 'the', 'closing', 'credits', 'to', 'europa', 'make', 'there', 'was', 'across', 'the', 'screen,', 'never', 'in', 'my', 'entire', 'life', 'have', 'i', 'seen', 'a', 'film', 'of', 'such', 'technical', 'genius,', 'the', 'visuals', 'of', 'europa', 'are', 'so', 'impressive', 'that', 'any', 'film', 'i', 'watch', 'in', \"it's\", 'wake', 'will', 'only', 'pale', 'in', 'comparison,', 'forget', 'your', 'michael', 'bay,', 'ridley', 'scott', 'slick', 'hollywood', 'cinematography,', 'europa', 'has', 'more', 'ethereal', 'beauty', 'than', 'anything', 'those', 'two', 'could', 'conjure', 'up', 'in', 'a', 'million', 'years.', 'now', \"i'd\", 'be', 'the', 'first', 'to', 'hail', 'lars', 'von', 'trier', 'a', 'genius', 'just', 'off', 'the', 'back', 'of', 'his', 'films', 'breaking', 'the', 'waves', 'and', 'dancer', 'in', 'the', 'dark,', 'but', 'this', 'is', 'stupid,', 'the', 'fact', 'that', 'europa', 'has', 'gone', 'un-noticed', 'by', 'film', 'experts', 'for', 'so', 'long', 'is', 'a', 'crime', 'against', 'cinema,', 'whilst', 'overrated', 'rubbish', 'like', 'crouching', 'tiger,', 'hidden', 'dragon', 'and', 'life', 'is', 'beautiful', 'clean', 'up', 'at', 'the', 'academy', 'awards', '(but', 'what', 'do', 'the', 'know)', 'europa', 'has', 'been', 'hidden', 'away,', 'absent', 'form', 'video', 'stores', 'and', '(until', 'recently)', 'any', 'british', 'tv', 'channels.', '<br', '/><br', '/>the', 'visuals', 'in', 'europa', 'are', 'not', 'mtv', 'gloss;', \"it's\", 'not', 'a', 'case', 'of', 'style', 'over', 'substance,', 'its', 'more', 'a', 'case', 'of', 'substance', 'dictating', 'style.', 'much', 'like', 'his', 'first', 'film', 'the', 'element', 'of', 'crime,', 'von', 'trier', 'uses', 'the', 'perspective', 'of', 'the', 'main', 'character', 'to', 'draw', 'us', 'into', 'his', 'world,', 'and', 'much', 'like', 'element,', 'the', 'film', 'begins', 'with', 'the', 'main', 'character', '(or', 'in', 'the', 'case', 'of', 'europa,', 'we', 'the', 'audience)', 'being', 'hypnotized.', 'as', 'we', 'move', 'down', 'the', 'tracks,', 'the', 'voice', 'of', 'the', 'narrator', '(max', 'von', 'sydow)', 'counts', 'us', 'down', 'into', 'a', 'deep', 'sleep,', 'until', 'we', 'awake', 'in', 'europa.', 'this', 'allows', 'von', 'trier', 'and', 'his', 'three', 'cinematographers', 'to', 'pay', 'with', 'the', 'conventions', 'of', 'time', 'and', 'imagery,', 'there', 'are', 'many', 'scenes', 'in', 'europa', 'when', 'a', 'character', 'in', 'the', 'background,', 'who', 'is', 'in', 'black', 'and', 'white,', 'will', 'interact', 'with', 'a', 'person', 'in', 'the', 'foreground', 'who', 'will', 'be', 'colour,', 'von', 'trier', 'is', 'trying', 'to', 'show', 'us', 'how', 'much', 'precedence', 'the', 'coloured', 'item', 'or', 'person', 'has', 'over', 'the', 'plot,', 'for', 'instance,', \"it's\", 'no', 'surprise', 'that', 'the', 'first', 'shot', 'of', 'leopold', 'kessler', '(jean-marc', 'barr)', 'is', 'in', 'colour,', 'since', 'he', 'is', 'the', 'only', 'character', \"who's\", 'actions', 'have', 'superiority', 'over', 'the', 'film.', '<br', '/><br', '/>the', 'performances', 'are', 'good,', 'they', 'may', 'not', 'be', 'on', 'par', 'with', 'performances', 'in', 'later', 'von', 'trier', 'films,', 'but', \"that's\", 'just', 'because', 'the', 'images', 'are', 'sometimes', 'so', 'distracting', 'that', 'you', \"don't\", 'really', 'pick', 'up', 'on', 'them', 'the', 'first', 'time', 'round.', 'but', 'i', 'would', 'like', 'to', 'point', 'out', 'the', 'fantastic', 'performance', 'of', 'jean-marc', 'barr', 'in', 'the', 'lead', 'role,', 'whose', 'blind', 'idealism', 'is', 'slowly', 'warn', 'down', 'by', 'the', 'two', 'opposing', 'sides,', 'until', 'he', 'erupts', 'in', 'the', 'films', 'final', 'act.', 'again,', 'muck', 'like', 'the', 'element', 'of', 'crime,', 'the', 'film', 'ends', 'with', 'our', 'hero', 'unable', 'to', 'wake', 'up', 'from', 'his', 'nightmare', 'state,', 'left', 'in', 'this', 'terrible', 'place,', 'with', 'only', 'the', 'continuing', 'narration', 'of', 'von', 'sydow', 'to', 'seal', 'his', 'fate.', 'europa', 'is', 'a', 'tremendous', 'film,', 'and', 'i', 'cant', 'help', 'thinking', 'what', 'a', 'shame', 'that', 'von', 'trier', 'has', 'abandoned', 'this', 'way', 'of', 'filming,', 'since', 'he', 'was', 'clearly', 'one', 'of', 'the', 'most', 'talented', 'visual', 'directors', 'working', 'at', 'that', 'time,', 'europa,', 'much', 'like', 'the', 'rest', 'of', 'his', 'cinematic', 'cannon', 'is', 'filled', 'with', 'a', 'wealth', 'of', 'iconic', 'scenes.', 'his', 'dedication', 'to', 'composition', 'and', 'mise-en-scene', 'is', 'unrivalled,', 'not', 'to', 'mention', 'his', 'use', 'of', 'sound', 'and', 'production', 'design.', 'but', 'since', 'his', 'no-frills', 'melodramas', 'turned', 'out', 'to', 'be', 'breaking', 'the', 'waves', 'and', 'dancer', 'in', 'the', 'dark', 'then', 'who', 'can', 'argue,', 'but', 'it', 'does', 'seems', 'like', 'a', 'waste', 'of', 'an', 'imaginative', 'talent.', '10/10'], 1)]\n",
      "[(['previous', 'reviewer', 'claudio', 'carvalho', 'gave', 'a', 'much', 'better', 'recap', 'of', 'the', \"film's\", 'plot', 'details', 'than', 'i', 'could.', 'what', 'i', 'recall', 'mostly', 'is', 'that', 'it', 'was', 'just', 'so', 'beautiful,', 'in', 'every', 'sense', '-', 'emotionally,', 'visually,', 'editorially', '-', 'just', 'gorgeous.<br', '/><br', '/>if', 'you', 'like', 'movies', 'that', 'are', 'wonderful', 'to', 'look', 'at,', 'and', 'also', 'have', 'emotional', 'content', 'to', 'which', 'that', 'beauty', 'is', 'relevant,', 'i', 'think', 'you', 'will', 'be', 'glad', 'to', 'have', 'seen', 'this', 'extraordinary', 'and', 'unusual', 'work', 'of', 'art.<br', '/><br', '/>on', 'a', 'scale', 'of', '1', 'to', '10,', \"i'd\", 'give', 'it', 'about', 'an', '8.75.', 'the', 'only', 'reason', 'i', 'shy', 'away', 'from', '9', 'is', 'that', 'it', 'is', 'a', 'mood', 'piece.', 'if', 'you', 'are', 'in', 'the', 'mood', 'for', 'a', 'really', 'artistic,', 'very', 'romantic', 'film,', 'then', \"it's\", 'a', '10.', 'i', 'definitely', 'think', \"it's\", 'a', 'must-see,', 'but', 'none', 'of', 'us', 'can', 'be', 'in', 'that', 'mood', 'all', 'the', 'time,', 'so,', 'overall,', '8.75.'], 1), (['contains', '\"spoiler\"', 'information.', 'watch', 'this', \"director's\", 'other', 'film,', '\"earth\",', 'at', 'some', 'point.', \"it's\", 'a', 'better', 'film,', 'but', 'this', 'one', \"isn't\", 'bad', 'just', 'different.<br', '/><br', '/>a', 'rare', 'feminist', 'point', 'of', 'view', 'from', 'an', 'indian', 'filmmaker.', 'tradition,', 'rituals,', 'duty,', 'secrets,', 'and', 'the', 'portrayal', 'of', 'strict', 'sex', 'roles', 'make', 'this', 'an', 'engaging', 'and', 'culturally', 'dynamic', 'film', 'viewing', 'experience.', 'all', 'of', 'the', 'married', 'characters', 'lack', 'the', '\"fire\"', 'of', 'the', 'marriage', 'bed', 'with', 'their', 'respective', 'spouses.', 'one', 'husband', 'is', 'celibate', 'and', 'commits', 'a', 'form', 'of', 'spiritual', '\"adultery\"', 'by', 'giving', 'all', 'of', 'his', 'love,', 'honor,', 'time', 'and', 'respect', 'to', 'his', 'religious', 'swami', '(guru).', 'his', 'wife', 'is', 'lonely', 'and', 'yearns', 'for', 'intimacy', 'and', 'tenderness', 'which', 'she', 'eventually', 'finds', 'with', 'her', 'closeted', 'lesbian', 'sister-in-law', 'who', 'comes', 'to', 'live', 'in', 'their', 'house', 'with', 'her', 'unfaithful', 'husband.', 'this', 'unfaithful', 'husband', 'is', 'openly', 'in', 'love', 'with', 'his', 'chinese', 'mistress', 'but', 'was', 'forced', 'into', 'marriage', 'with', 'a', '(unbeknownest', 'to', 'him)', 'lesbian.', 'they', 'only', 'have', 'sex', 'once', 'when', 'his', 'closet', 'lesbian', 'wife', 'loses', 'her', 'virginity.<br', '/><br', '/>a', 'servant', 'lives', 'in', 'the', 'house', 'and', 'he', 'eventually', 'reveals', 'the', 'secret', 'that', 'the', 'two', 'women', 'are', 'lovers.', 'another', 'significant', 'character', 'is', 'the', 'elderly', 'matriarch', 'who', 'is', 'unable', 'to', 'speak', 'or', 'care', 'for', 'herself', 'due', 'to', 'a', 'stroke.', 'however,', 'she', 'uses', 'a', 'ringing', 'bell', 'to', 'communicate', 'her', 'needs', 'as', 'well', 'as', 'her', 'displeasure', 'with', 'the', 'family', 'members.', 'she', 'lets', 'them', 'know', 'through', 'her', 'bell', 'or', 'by', 'pounding', 'her', 'fist', 'that', 'she', 'knows', 'exacly', \"what's\", 'going', 'on', 'in', 'the', 'house', 'and', 'how', 'much', 'she', 'disapproves.<br', '/><br', '/>in', 'the', 'end,', 'the', 'truth', 'about', 'everybody', 'comes', 'out', 'and', 'the', 'two', 'female', 'lovers', 'end', 'up', 'running', 'away', 'together.', 'but,', 'not', 'before', 'there', 'is', 'an', 'emotional', 'scene', 'between', 'the', 'swami-addicted', 'husband', 'and', 'his', 'formerly', 'straight', 'wife.', 'her', 'sari', 'catches', 'on', 'fire', 'and', 'at', 'first', 'we', 'think', 'she', 'is', 'going', 'to', 'die.', 'however,', 'we', 'see', 'the', 'two', 'women', 'united', 'in', 'the', 'very', 'last', 'scene', 'of', 'the', 'movie.<br', '/><br', '/>the', 'writer/director', 'of', 'this', 'film', 'challenges', 'her', \"culture's\", 'traditions,', 'but', 'she', 'shows', 'us', 'individual', 'human', 'beings', 'who', 'are', 'trapped', 'by', 'their', 'culture', 'and', 'gender.', 'we', 'come', 'to', 'really', 'care', 'about', 'the', 'characters', 'and', 'we', \"don't\", 'see', 'them', 'as', 'stereotypes.', 'each', 'on', 'surprises', 'us', 'with', 'their', 'humanity,', 'vulgarity,', 'tenderness,', 'anger,', 'and', 'spirit.'], 1), (['this', 'is', 'my', 'first', 'deepa', 'mehta', 'film.', 'i', 'saw', 'the', 'film', 'on', 'tv', 'in', 'its', 'hindi', 'version', 'with', 'its', '\"sita\"', 'character', 'presented', 'as', 'nita.', 'i', 'also', 'note', 'that', 'it', 'is', 'radha', 'who', 'underwent', 'the', 'allegorical', 'trial', 'by', 'fire', 'in', 'the', 'film', 'and', 'not', 'nita/sita.', 'yet', 'what', 'i', 'loved', 'about', 'the', 'film', 'was', 'its', 'screenplay', 'by', 'ms', 'mehta,', 'not', 'her', 'direction.', 'the', 'characters,', 'big', 'and', 'small,', 'were', 'well-developed', 'and', 'seemed', 'quixotic', 'towards', 'the', 'end--somewhat', 'like', 'the', 'end', 'of', \"mazursky's\", '\"an', 'unmarried', 'woman.\"', 'they', 'are', 'brave', 'women', 'surrounded', 'by', 'cardboard', 'men.', 'and', 'one', 'cardboard', 'man', '(ashok)', 'seems', 'to', 'come', 'alive', 'in', 'the', 'last', 'shot', 'we', 'see', 'of', 'him---carrying', 'his', 'invalid', 'mother', 'biji.', 'he', 'seems', 'to', 'finally', 'take', 'on', 'a', 'future', 'responsibility', 'beyond', 'celibacy', 'and', 'adherance', 'to', 'religion.', '<br', '/><br', '/>ms', 'mehta', 'seems', 'to', 'fumble', 'as', 'a', 'director', '(however,', 'compared', 'to', 'most', 'indian', 'mainstream', 'cinema', 'she', 'would', 'seem', 'to', 'be', 'brilliant)', 'as', 'she', 'cannot', 'use', 'her', 'script', 'to', 'go', 'beyond', 'the', 'microscopic', 'joint', 'family', 'she', 'is', 'presenting', 'except', 'presenting', 'a', 'glimpse', 'of', 'the', 'chinese', 'micro-minority', 'in', 'the', 'social', 'milieu', 'of', 'india.', 'she', 'even', 'dedicates', 'the', 'film', 'to', 'her', 'mother', 'and', 'daughter', '(not', 'her', 'father!)', 'yet', 'her', 'radha', 'reminesces', 'of', 'halcyon', 'days', 'with', 'both', 'her', 'parents', 'in', 'a', 'mustard', 'field.', 'compare', 'her', 'to', 'mrinal', 'sen,', 'adoor', 'gopalakrishnan,', 'muzaffar', 'ali', 'and', 'she', 'is', 'dwarfed', 'by', 'these', 'giants--given', 'her', 'competent', 'canadian', 'production', 'team', 'and', 'financial', 'resources!', \"mehta's\", 'film', 'of', 'two', 'bisexual', 'ladies', 'in', 'an', 'indian', 'middle-class', 'household', 'may', 'be', 'sacrilege', 'to', 'some,', 'but', 'merely', 'captures', 'the', 'atrophy', 'of', 'middle-class', 'homes', 'that', 'does', 'not', 'seem', 'to', 'aspire', 'for', 'something', 'better', 'than', 'its', 'immediate', 'survival', 'in', 'a', 'limited', 'social', 'space.', 'kannada,', 'malayalam,', 'and', 'bengali', 'films', 'have', 'touched', 'parallel', 'themes', 'in', 'india', 'but', 'did', 'not', 'have', 'the', 'publicity', 'that', 'surrounded', 'this', 'film', 'and', 'therefore', 'have', 'not', 'been', 'seen', 'by', 'a', 'wide', 'segment', 'of', 'knowledgeable', 'cinemagoers.<br', '/><br', '/>ms', 'das,', 'ms', 'azmi,', 'mr', 'jafri', 'and', 'mr', 'kharbanda', 'are', 'credible', 'but', 'not', 'outstanding.', 'ms', 'azmi', 'is', 'a', 'talented', 'actress', 'who', 'gave', 'superb', 'performances', 'under', 'good', 'directors', '(mrinal', \"sen's\", '\"khandar\",', 'gautam', \"ghose's\", '\"paar\",', \"benegal's\", '\"ankur\")', 'a', 'brilliance', 'notably', 'absent', 'in', 'this', 'film.', 'ms', 'das', 'sparkled', 'due', 'to', 'her', 'screen', 'presence', 'rather', 'than', 'her', 'acting', 'capability.', 'all', 'in', 'all,', 'the', \"film's\", 'strength', 'remains', 'in', 'the', 'structure', 'of', 'the', 'screenplay', 'which', 'is', 'above', 'average', 'in', 'terms', 'of', 'international', 'cinema.', 'i', 'am', 'sure', 'ms', 'mehta', 'can', 'hone', 'her', 'writing', 'talents', 'in', 'her', 'future', 'screenplays.'], 1), (['this', 'was', 'a', 'great', 'film', 'in', 'every', 'sense', 'of', 'the', 'word.', 'it', 'tackles', 'the', 'subject', 'of', 'tribadism', 'in', 'a', 'society', 'that', 'is', 'quite', 'intolerant', 'of', 'any', 'deviations', 'from', 'the', 'norm.', 'it', 'criticises', 'a', 'great', 'many', 'indian', 'customs', 'that', 'many', 'find', 'oppressive', '--', 'such', 'as', 'the', 'arranging', 'of', 'marriages', 'by', 'others,', 'the', 'importance', 'of', 'status', 'and', 'face,', 'religious', 'hypocrisy,', 'sexism,', 'the', 'valuation', 'of', 'women', 'in', 'terms', 'of', 'their', 'baby-making', 'capacity,', 'the', 'binding', 'concepts', 'of', 'duty', 'and', 'so', 'on.', 'at', 'the', 'heart', 'of', 'the', 'film', 'is', 'a', 'touching', 'love', 'story', 'that', 'goes', 'beyond', 'such', 'limitations', 'of', 'the', 'society', 'which', 'the', 'two', 'protagonists', 'find', 'themselves.', 'the', 'film', 'is', 'well-acted', 'and', 'genuine,', 'completely', 'believable', 'from', 'beginning', 'to', 'end,', 'unlike', 'most', 'bollywood', 'flicks.', 'the', 'main', 'faults', 'of', 'the', 'film', 'as', 'i', 'saw', 'it', 'was', 'first,', 'that', 'the', 'two', 'lovers', 'seem', 'drawn', 'to', 'one', 'another', 'not', 'necessarily', 'by', 'a', 'natural', 'affinity', 'for', 'each', 'other', 'as', 'much', 'as', 'the', 'fact', 'that', 'they', 'are', 'stuck', 'in', 'dead-end', 'marriages', 'with', 'no', 'passion', 'and', 'no', 'rewards.', 'this', 'may', 'play', 'a', 'part', 'in', 'the', 'sexual', 'awakening', 'of', 'the', 'characters,', 'but', 'most', 'people', 'stuck', 'in', 'the', 'same', 'situation', 'will', 'not', '\"turn', 'homosexual\".', 'it', 'seems', 'clear', 'from', 'the', 'beginning', 'of', 'the', 'film', 'that', 'the', 'two', 'characters', 'are', 'quite', 'heterosexual', '--', 'when', 'radha', 'does', 'her', 'scene', 'at', 'the', 'end', 'of', 'the', 'movie', 'with', 'aashok,', 'she', 'makes', 'it', 'quite', 'clear', 'that', '\"without', 'desire', 'she', 'was', 'dead\",', 'and', 'the', 'implication', 'was', 'that', 'if', 'he', 'had', 'desired', 'so,', 'he', 'could', 'have', 'fulfilled', 'her', 'quite', 'completely,', 'and', 'also', 'when', 'sita', 'seemed', 'very', 'disappointed', 'when', 'her', 'husband', 'seemed', 'to', 'not', 'like', 'her.', 'such', 'situations', 'do', 'not', 'turn', 'people', 'into', 'homosexuals', '--', 'they', 'may', 'seek', 'comfort', 'in', 'others', 'in', 'the', 'same', 'position,', 'but', 'inthe', 'film', 'it', 'is', 'not', 'at', 'all', 'made', 'clear', 'that', 'they', 'are', 'lesbians', 'from', 'the', 'beginning', '--', 'quite', 'the', 'opposite.', 'some', 'people', 'are', 'bisexual,', 'it', 'is', 'true,', 'but', 'most', 'tend', 'to', 'be', 'either', 'hetero-', 'or', 'homosexual.', 'in', 'the', 'case', 'of', 'the', 'ladies', 'in', 'the', 'film,', 'both', 'had', 'insensitive', 'jerks', 'for', 'husbands', '.', '.', '.', 'if', 'this', 'had', 'not', 'been', 'the', 'case,', 'would', 'they', 'have', 'naturally', 'found', 'the', 'need', 'to', 'express', 'their', 'desire', 'in', 'a', 'relationship', 'that', 'they', 'may', 'have', 'otherwise', 'not', 'have', 'considered?', 'the', 'film', 'ignores', 'this.', 'the', 'other', 'fault', 'is', 'the', 'naming', 'of', 'the', 'characters', '.', '.', '.', 'the', 'names', 'sita', 'and', 'radha', 'seem', 'contrived', 'deliberately', 'to', 'shock', 'and', 'outrage', '(imagine', 'a', 'film', 'in', 'america', 'depicting', 'a', 'gay', 'relationship', 'between', 'a', 'man', 'named', '\"jesus\"', 'and', 'another', 'named', '\"paul\"!)', 'by', 'using', 'names', 'associated', 'with', 'various', 'hindoo', 'scriptures.', 'the', 'film', 'is', 'strong', 'enough', 'to', 'stand', 'on', 'its', 'own', 'and', 'needs', 'no', 'such', 'devices', 'in', 'my', 'opinion.', 'at', 'any', 'rate,', 'the', 'faults', 'do', 'not', 'take', 'much', 'away', 'from', 'the', 'power', 'of', 'the', 'movie.', 'it', 'is', 'indeed', 'a', 'very', 'touching', 'and', 'powerful', 'story', '--', 'the', 'images', 'and', 'characters', 'will', 'stay', 'with', 'you', 'a', 'long', 'time', 'after', 'you', 'leave', 'the', 'theatre.'], 1), (['a', 'stunningly', 'well-made', 'film,', 'with', 'exceptional', 'acting,', 'directing,', 'writing,', 'and', 'photography.<br', '/><br', '/>a', 'newlywed', 'finds', 'married', 'life', 'not', 'what', 'she', 'expected,', 'and', 'starts', 'to', 'question', 'her', 'duty', 'to', 'herself', 'versus', 'her', 'duty', 'to', 'society.', 'together', 'with', 'her', 'sister', '-in-law,', 'she', 'makes', 'some', 'radical', 'departures', 'from', 'conventional', 'roles', 'and', 'mores.'], 1)]\n"
     ]
    }
   ],
   "source": [
    "def data_preprocess(corpus):\n",
    "    data_set = []\n",
    "    for sentence, sentence_label in corpus:\n",
    "        # 这里有一个小trick是把所有的句子转换为小写，从而减小词表的大小\n",
    "        # 一般来说这样的做法有助于效果提升\n",
    "        sentence = sentence.strip().lower()\n",
    "        sentence = sentence.split(\" \")\n",
    "        \n",
    "        data_set.append((sentence, sentence_label))\n",
    "\n",
    "    return data_set\n",
    "\n",
    "train_corpus = data_preprocess(train_corpus)\n",
    "test_corpus = data_preprocess(test_corpus)\n",
    "print(train_corpus[:5])\n",
    "print(test_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在经过切词后，需要构造一个词典，把每个词都转化成一个ID，以便于神经网络训练。代码如下：\n",
    "\n",
    "------\n",
    "**注意：**\n",
    "\n",
    "在代码中我们使用了一个特殊的单词\"[oov]\"（out-of-vocabulary），用于表示词表中没有覆盖到的词。之所以使用\"[oov]\"这个符号，是为了处理某一些词，在测试数据中有，但训练数据没有的现象。\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 252173 different words in the corpus\n",
      "word [oov], its id 0, its word freq 10000000000\n",
      "word [pad], its id 1, its word freq 10000000000\n",
      "word the, its id 2, its word freq 322174\n",
      "word a, its id 3, its word freq 159949\n",
      "word and, its id 4, its word freq 158556\n",
      "word of, its id 5, its word freq 144459\n",
      "word to, its id 6, its word freq 133965\n",
      "word is, its id 7, its word freq 104170\n",
      "word in, its id 8, its word freq 90521\n",
      "word i, its id 9, its word freq 70477\n"
     ]
    }
   ],
   "source": [
    "# 构造词典，统计每个词的频率，并根据频率将每个词转换为一个整数id\n",
    "def build_dict(corpus):\n",
    "    word_freq_dict = dict()\n",
    "    for sentence, _ in corpus:\n",
    "        for word in sentence:\n",
    "            if word not in word_freq_dict:\n",
    "                word_freq_dict[word] = 0\n",
    "            word_freq_dict[word] += 1\n",
    "\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    word2id_dict = dict()\n",
    "    word2id_freq = dict()\n",
    "\n",
    "    # 一般来说，我们把oov和pad放在词典前面，给他们一个比较小的id，这样比较方便记忆，并且易于后续扩展词表\n",
    "    word2id_dict['[oov]'] = 0\n",
    "    word2id_freq[0] = 1e10\n",
    "\n",
    "    word2id_dict['[pad]'] = 1\n",
    "    word2id_freq[1] = 1e10\n",
    "\n",
    "    for word, freq in word_freq_dict:\n",
    "        word2id_dict[word] = len(word2id_dict)\n",
    "        word2id_freq[word2id_dict[word]] = freq\n",
    "\n",
    "    return word2id_freq, word2id_dict\n",
    "\n",
    "word2id_freq, word2id_dict = build_dict(train_corpus)\n",
    "vocab_size = len(word2id_freq)\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "for _, (word, word_id) in zip(range(10), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成word2id词典假设之后，我们还需要进一步处理原始语料，把语料中的所有句子都处理成ID序列，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 tokens in the corpus\n",
      "[([22216, 41, 76, 8, 1136, 17, 2, 874, 979, 167, 69425, 24, 283, 707, 2, 19881, 5, 16628, 11952, 37, 100421, 52, 7, 76, 5733, 415, 912, 52, 7, 32, 1426, 299, 36, 195, 2299, 644, 17, 3, 282, 27, 141, 61, 7447, 4, 555, 25364, 7, 35, 2, 51, 3590, 8, 2691, 17, 2, 69426, 13, 688, 428, 26, 6, 142, 11, 2, 874, 160, 41, 3, 51, 14841, 4458, 22216, 7, 3, 218, 6262, 8, 10, 6919, 382, 10, 7, 100422, 12, 7, 1394, 15, 3, 100423, 4, 242, 12, 104, 5041, 54, 2368, 2, 4828, 109, 13, 255, 20, 7, 32280, 100424, 8, 51, 68, 30, 29571, 30, 102, 1010, 2, 4142, 18952, 11069, 18, 11636, 4, 12644], 1), ([22216, 7, 2, 78, 225, 20, 190, 119, 8, 1043, 46, 25, 37, 1008, 4578, 11, 22, 4379, 31, 24, 9244, 96, 10, 7, 39, 2, 246, 5601, 16, 35, 5, 136, 385, 1901, 11953, 69427, 2, 3689, 124, 2351, 2666, 17339, 100425, 37, 2732, 2, 6821, 26, 1702, 51, 35630, 18, 10, 7, 61, 21, 116, 912, 12, 7, 7006, 191, 99, 6263, 4, 1485, 2, 439, 2239, 5, 1221, 4, 513, 2598, 44, 104, 97, 27, 761, 32281, 5417, 66, 2, 3893, 5, 2, 3263, 44, 261, 6, 142, 36, 3690, 101, 285, 34, 2, 507, 27, 13410, 611, 18, 44, 4535, 2398, 11, 27, 376, 72, 21, 2692, 25365, 15379, 583, 2649], 1), ([9839, 2666, 8704, 7, 102, 16629, 8, 243, 47, 154, 22217, 45, 5, 115, 22, 49, 225, 133, 498, 22, 116, 35631, 13, 3779, 4730, 16628, 3630, 15, 3, 9428, 1621, 10296, 17, 38, 100, 7214, 3515, 8, 27295, 6442, 20987, 3, 185, 299, 5, 1155, 7448, 673, 18114, 6, 323, 8, 59, 54956, 12, 7, 23, 3, 693, 3660, 15, 27, 1048, 554, 2877, 13, 7328, 1882, 554, 101, 3, 331, 15, 3, 394, 10539, 19, 2, 22216, 14842, 3720, 21, 331, 7, 6, 5370, 6, 2, 676, 5, 2, 46152, 50, 2, 5663, 22, 6264, 3, 13411, 1132, 7, 95, 19, 2, 100426, 3, 495, 4829, 10798, 50, 3, 54957, 5663, 22, 23, 100427, 446, 2, 231, 33, 26, 71, 40128, 52, 22, 100, 13016, 6, 2, 1155, 35632, 5, 32282, 6, 130, 467, 29572, 13, 93, 14842, 1537, 7, 37, 32, 13809, 9626, 1535, 46153, 140, 161, 17, 35, 83, 7108, 4, 46154, 8, 30, 809, 6442, 27296, 140, 2, 149, 69428, 6, 1874, 115, 1079, 17, 100428, 3122, 3608, 6, 26, 39, 4179, 34, 54958, 421, 124, 8126, 668, 15, 33, 1927, 22, 10799, 15, 8, 3, 9044, 233, 17, 83, 209, 495, 1769, 266, 100429, 13, 2352, 3, 2255, 437, 100430, 6442, 2994, 34, 2, 1621, 17, 3, 100431, 11351, 8, 797, 5, 100, 54959, 27, 11954, 2, 3161, 6, 2, 40129, 5, 3, 69429, 27, 1883, 6, 21, 3609, 4, 146, 3, 2123, 6, 3, 84, 35633, 370, 27, 2994, 34, 2, 1621, 2599, 317, 69430, 4, 1917, 8, 2, 845, 9429, 19, 3, 2618, 16630, 275, 29, 2, 482, 801, 27, 973, 11, 21, 331, 7, 6, 1712, 4, 23, 69431, 8871, 15, 27, 1010, 2, 1621, 7329, 3, 1468, 4056, 27, 1108, 29, 40130, 2798, 6, 2667, 2, 1621, 4, 595, 2, 35633, 46, 25, 456, 6, 27297, 2, 1173, 12, 7, 3, 302, 1240, 13017, 168, 895, 8, 6083, 12, 90, 58, 783, 8, 3, 20988, 13, 792, 694, 79, 30, 7449, 105, 1724, 130, 15, 3, 5371, 5, 27298, 2378, 19, 12645, 4, 27299, 285, 17, 2, 23700, 5, 2, 32283, 13, 3837, 2, 231, 11, 10, 24, 7, 4980, 46155, 9, 2845, 9839, 2666, 17339, 370, 129, 100432, 2, 6821, 4, 2, 69432, 2, 3161, 150, 2152, 801, 61, 253, 87, 1308, 382, 145, 3, 69433], 1), ([69434, 2399, 610, 6, 87, 240, 6, 1613, 45, 24, 17340, 38, 308, 29, 111, 209, 100433, 13, 165, 403, 10, 139, 2, 316, 5, 46156, 8, 45, 5, 2, 159, 146, 12, 268, 37, 32, 170, 10297, 208, 6, 506, 9, 81, 447, 2, 15380, 5, 89, 19, 3, 149, 1175, 16, 2890, 8, 30, 971, 11955, 2030, 21, 378, 4, 53, 7, 9627, 470, 101, 228, 68, 8, 3, 716, 3569, 1175, 42, 468, 7, 588, 8, 2, 149, 4, 23701, 7, 8, 2, 54960, 13, 1274, 5, 2, 116, 988, 5, 10, 7, 50, 2, 185, 1197, 494, 2, 100434, 19, 2, 10800, 3, 150, 339, 8, 32, 202, 954, 4459, 2329, 17, 2678, 547, 27300, 12, 1457, 87, 5, 35634, 54961, 11, 1760, 1750, 507, 6177, 17, 2, 14843, 5, 825, 6, 2909, 3414, 5, 213, 191, 76, 5093, 163, 17, 2, 150, 50, 40131, 554, 42, 410, 460, 8, 2, 100435, 64, 58, 67, 2, 825, 1522, 19, 2, 4880, 12, 7, 30, 5, 2, 194, 9, 120, 43, 2666, 69435, 21, 23702, 5, 2846, 63, 654, 65, 242, 12, 35, 2, 51, 32284, 13, 93, 316, 5, 3805, 8, 2, 24, 14, 7705, 37, 50, 23701, 122, 7, 3691, 31, 2, 4830, 100436, 9, 26, 102, 119, 237, 37, 11, 8, 3, 748, 13, 93, 316, 5, 328, 289, 8, 10, 24, 14, 107, 1682, 100437, 69436, 7, 88, 15, 23701, 18115, 12, 857, 65, 2, 69437, 9, 176, 34, 21, 263, 8, 2, 6716, 100438, 10, 641, 146, 2, 18116, 5, 2256, 15381, 100439, 5, 1621, 100440, 375, 2, 495, 5262, 5, 11070, 4, 35, 2, 23703, 3927, 6, 12301, 12, 1644, 87, 5, 25366, 8, 2, 108, 2, 2050, 54962, 7, 243, 6, 4611, 3, 20989, 29, 2, 172, 5, 182, 30, 50, 292, 7, 162, 7450, 12, 287, 11, 45, 89, 22, 319, 10050, 6, 2, 552, 194, 2022, 197, 429, 266, 45, 1051, 37, 19882, 22, 8705, 1441, 18, 85, 164, 1117, 43, 619, 13, 93, 642, 2780, 247, 6920, 107, 31, 2962, 2666, 46157, 7, 1383, 799, 12, 3751, 25, 72, 2, 75, 4, 146, 25, 2051, 72, 23701, 3692, 63, 29, 286, 7, 3, 861, 320, 6, 5418, 13, 93, 20, 857, 65, 2, 249, 11, 30, 7, 3, 11071, 46, 33, 86, 2210, 3, 4880, 9, 67, 2, 163, 321, 309, 8, 4928, 8, 2, 2650, 106, 54963, 122, 141, 1247, 65, 16, 907, 4, 654, 65, 99, 42, 209, 11637, 2781, 29, 30, 390, 2666, 8704, 401, 100441, 6, 493, 31, 242, 2732, 2, 6821, 4, 14298, 18, 203, 9, 118, 21, 950, 121, 86, 3516, 107, 16631, 9, 236, 608, 29, 2, 163, 320, 276, 2, 172, 5, 14298, 50, 25, 1106, 171, 2, 258, 122, 6, 19883, 59, 10540, 18, 5, 532, 33, 102, 14299], 1), ([11, 14, 2, 82, 174, 11, 46158, 6, 432, 15, 9, 273, 2, 2926, 1137, 6, 9245, 91, 52, 14, 563, 2, 2046, 102, 8, 57, 369, 161, 26, 9, 119, 3, 24, 5, 130, 1660, 7848, 2, 2978, 5, 9245, 22, 38, 1409, 11, 92, 24, 9, 110, 8, 44, 3752, 74, 58, 7573, 8, 15382, 885, 111, 481, 29573, 15978, 1223, 6084, 385, 2494, 9245, 41, 51, 14844, 1140, 68, 237, 136, 103, 90, 16632, 65, 8, 3, 1530, 1043, 203, 428, 28, 2, 82, 6, 13810, 9839, 2666, 8704, 3, 1792, 39, 137, 2, 149, 5, 21, 129, 2732, 2, 6821, 4, 4928, 8, 2, 2650, 18, 10, 7, 2353, 2, 231, 11, 9245, 41, 908, 69438, 31, 24, 12302, 16, 38, 229, 7, 3, 997, 375, 2443, 1828, 5311, 3280, 37, 15979, 13811, 1572, 3957, 4, 161, 7, 315, 2891, 65, 29, 2, 1864, 2799, 2112, 48, 85, 2, 20990, 9245, 41, 71, 1572, 2197, 7109, 926, 433, 8412, 4, 20991, 69439, 92, 632, 280, 19884, 109, 13, 93, 2978, 8, 9245, 22, 23, 6532, 100442, 44, 23, 3, 529, 5, 507, 127, 10298, 83, 51, 3, 529, 5, 3631, 27301, 3304, 76, 37, 21, 82, 24, 2, 1498, 5, 6533, 2666, 8704, 988, 2, 2927, 5, 2, 258, 122, 6, 2529, 188, 72, 21, 1437, 4, 76, 37, 10801, 2, 24, 763, 17, 2, 258, 122, 875, 8, 2, 529, 5, 35635, 69, 2, 18953, 99, 46159, 15, 69, 858, 214, 2, 25367, 2, 642, 5, 2, 5792, 35636, 2666, 69440, 10051, 188, 214, 72, 3, 991, 11956, 313, 69, 6265, 8, 35637, 10, 1952, 2666, 8704, 4, 21, 284, 32285, 6, 966, 17, 2, 7574, 5, 84, 4, 10802, 52, 22, 100, 159, 8, 9245, 50, 3, 122, 8, 2, 7849, 36, 7, 8, 328, 4, 4057, 74, 7451, 17, 3, 450, 8, 2, 19885, 36, 74, 28, 18117, 2666, 8704, 7, 243, 6, 134, 188, 79, 76, 69441, 2, 32286, 8127, 40, 450, 41, 127, 2, 728, 16, 2890, 44, 60, 1200, 11, 2, 82, 339, 5, 11955, 20987, 54964, 46160, 7, 8, 18117, 227, 27, 7, 2, 58, 122, 808, 2184, 26, 19886, 127, 2, 139, 109, 13, 93, 423, 22, 523, 33, 186, 23, 28, 19, 3415, 17, 423, 8, 370, 2666, 8704, 580, 18, 189, 39, 77, 2, 1385, 22, 501, 38, 6263, 11, 25, 86, 61, 1281, 65, 19, 115, 2, 82, 84, 18118, 18, 9, 54, 37, 6, 249, 47, 2, 936, 269, 5, 22218, 19887, 8, 2, 475, 1469, 555, 2268, 18954, 7, 1512, 3006, 214, 31, 2, 103, 11957, 16633, 313, 27, 20992, 8, 2, 129, 435, 3281, 620, 29574, 37, 2, 1498, 5, 6533, 2, 24, 654, 17, 241, 887, 1965, 6, 3752, 65, 34, 21, 2308, 7452, 291, 8, 10, 495, 2221, 17, 58, 2, 7110, 3448, 5, 2666, 22219, 6, 13412, 21, 10541, 9245, 7, 3, 3753, 151, 4, 9, 2429, 323, 545, 48, 3, 1120, 11, 2666, 8704, 41, 2800, 10, 108, 5, 10542, 227, 27, 14, 631, 30, 5, 2, 78, 1039, 1036, 998, 755, 29, 11, 390, 35635, 76, 37, 2, 326, 5, 21, 1255, 8269, 7, 1079, 17, 3, 5199, 5, 6085, 1125, 21, 11352, 6, 10299, 4, 35638, 7, 100443, 23, 6, 710, 21, 316, 5, 479, 4, 411, 13812, 18, 227, 21, 69442, 20993, 599, 47, 6, 28, 2732, 2, 6821, 4, 4928, 8, 2, 513, 96, 36, 64, 25368, 18, 12, 114, 175, 37, 3, 393, 5, 32, 3894, 3517, 3388], 1)]\n",
      "[([868, 2313, 29392, 0, 442, 3, 76, 138, 20739, 5, 2, 502, 131, 1724, 68, 9, 9002, 48, 9, 2606, 636, 7, 11, 12, 14, 39, 38, 2338, 8, 158, 318, 70, 17859, 14000, 0, 70, 39, 0, 13, 558, 25, 37, 123, 11, 22, 400, 6, 166, 5447, 4, 81, 26, 869, 1976, 6, 63, 11, 1140, 7, 92312, 9, 98, 25, 74, 28, 1182, 6, 26, 119, 10, 3195, 4, 1865, 210, 5, 13350, 13, 2435, 3, 3873, 5, 815, 6, 3669, 428, 183, 12, 43, 32, 0, 2, 58, 307, 9, 4506, 285, 34, 1802, 7, 11, 12, 7, 3, 1519, 6128, 46, 25, 22, 8, 2, 1519, 16, 3, 61, 21792, 49, 714, 151, 96, 44, 3, 1317, 9, 355, 98, 44, 3, 39542, 18, 603, 5, 188, 64, 28, 8, 11, 1519, 35, 2, 390, 767, 1671, 0], 1), ([1292, 90413, 11644, 110, 10, 2004, 80, 151, 0, 29, 45, 1912, 44, 3, 138, 151, 18, 10, 30, 205, 97, 39, 12004, 13, 742, 1272, 7050, 249, 5, 797, 34, 32, 1382, 10638, 16256, 90851, 21795, 42797, 4, 2, 1128, 5, 8494, 454, 841, 91, 10, 32, 2224, 4, 26441, 4321, 24, 987, 2531, 35, 5, 2, 1116, 121, 504, 2, 68873, 5, 2, 1756, 1987, 17, 59, 5583, 49133, 30, 828, 7, 96790, 4, 7250, 3, 926, 5, 4004, 0, 31, 647, 35, 5, 21, 1448, 19126, 84, 4, 1387, 6, 21, 1768, 0, 0, 21, 378, 7, 3015, 4, 19969, 16, 14407, 4, 15421, 63, 53, 853, 554, 17, 42, 31505, 2741, 37003, 36, 239, 6, 422, 8, 59, 412, 17, 42, 15064, 5542, 10, 15064, 828, 7, 8971, 8, 120, 17, 21, 1687, 7032, 18, 14, 923, 72, 1756, 17, 3, 0, 6, 13450, 28289, 33, 58, 26, 454, 295, 50, 21, 6448, 2741, 378, 1900, 42, 251602, 13, 742, 7812, 517, 8, 2, 412, 4, 27, 853, 2594, 2, 1093, 11, 2, 103, 418, 22, 10682, 167, 2743, 122, 7, 2, 3937, 33049, 36, 7, 1965, 6, 1247, 40, 456, 16, 907, 610, 6, 3, 31531, 248, 53, 988, 3, 16162, 6675, 6, 6534, 42, 676, 15, 107, 15, 42, 14154, 17, 2, 256, 10056, 53, 1620, 115, 118, 140, 42, 6675, 40, 31, 17608, 42, 7054, 11, 53, 697, 0, 777, 162, 19, 8, 2, 412, 4, 79, 76, 53, 0, 13, 537, 2, 656, 2, 1205, 43, 1602, 239, 47, 4, 2, 103, 618, 2545, 172, 65, 578, 285, 1374, 712, 23, 179, 52, 7, 32, 869, 150, 187, 2, 0, 828, 4, 21, 12564, 812, 2959, 42, 202678, 4145, 19, 1340, 4, 29, 82, 69, 98, 53, 7, 162, 6, 5186, 248, 69, 67, 2, 103, 418, 2362, 8, 2, 49, 224, 150, 5, 2, 708, 13, 93, 3845, 5, 10, 24, 6216, 42, 55389, 25766, 18, 53, 287, 188, 2485, 396, 5133, 36, 22, 2768, 31, 59, 1701, 4, 24850, 69, 208, 6, 61, 456, 43, 2, 121, 4, 69, 86, 67, 115, 15, 10190, 233, 19, 3552, 188, 17, 59, 10636, 57263, 35711, 9493, 4, 8425], 1), ([10, 7, 57, 82, 216402, 0, 139, 9, 200, 2, 24, 19, 280, 8, 83, 6894, 324, 17, 83, 0, 122, 1394, 15, 0, 9, 81, 1271, 11, 12, 7, 20940, 36, 40892, 2, 18277, 4302, 31, 1340, 8, 2, 24, 4, 23, 0, 266, 48, 9, 403, 43, 2, 24, 14, 83, 1088, 31, 8022, 216403, 23, 42, 3123, 2, 646, 196, 4, 7665, 66, 20084, 4, 401, 155068, 854, 2, 0, 37, 2, 172, 5, 0, 11022, 33942, 61182, 33, 22, 2955, 418, 3483, 31, 3801, 3701, 4, 30, 3801, 160, 0, 175, 6, 208, 1989, 8, 2, 224, 339, 69, 67, 5, 0, 21, 58262, 542, 0, 27, 175, 6, 449, 180, 19, 3, 910, 5816, 675, 244102, 4, 0, 6, 8826, 109, 13, 80128, 0, 175, 6, 41315, 15, 3, 201, 31541, 1025, 6, 78, 1382, 2765, 627, 53, 54, 268, 6, 28, 150953, 15, 53, 486, 316, 42, 272, 6, 135, 675, 2, 73962, 10695, 256, 53, 7, 5535, 557, 5535, 3, 3067, 5, 2, 1687, 0, 8, 2, 994, 23108, 5, 11517, 53, 55, 46471, 2, 24, 6, 42, 542, 4, 778, 1472, 42, 0, 266, 42, 20940, 0, 5, 149611, 734, 17, 193, 42, 1018, 8, 3, 47624, 12488, 1633, 42, 6, 0, 84558, 0, 0, 0, 10000, 4, 53, 7, 43092, 31, 124, 0, 42, 3863, 2427, 411, 921, 4, 4209, 0, 191428, 24, 5, 103, 20338, 2391, 8, 32, 1382, 9789, 6237, 186, 28, 56515, 6, 7465, 18, 1400, 2200, 2, 0, 5, 9789, 9049, 11, 114, 23, 268, 6, 16896, 16, 143, 138, 68, 83, 5450, 5692, 8, 3, 1806, 994, 7079, 0, 0, 4, 0, 129, 26, 2957, 5328, 1521, 8, 4817, 18, 113, 23, 26, 2, 8244, 11, 3483, 10, 24, 4, 1934, 26, 23, 71, 119, 31, 3, 2205, 2614, 5, 17554, 0, 13, 80128, 97518, 8022, 0, 1722, 44063, 4, 1722, 125301, 22, 3917, 18, 23, 7588, 8022, 54565, 7, 3, 1039, 633, 36, 442, 1096, 423, 441, 56, 998, 0, 216054, 0, 241343, 0, 0, 0, 0, 3, 4850, 4134, 7109, 8, 10, 139, 8022, 26168, 0, 610, 6, 42, 387, 1558, 228, 68, 42, 132, 78795, 35, 8, 364, 2, 502, 2434, 1252, 8, 2, 3458, 5, 2, 1088, 63, 7, 801, 953, 8, 1403, 5, 1902, 1811, 9, 226, 274, 8022, 0, 64, 53472, 42, 553, 2447, 8, 42, 910, 59369], 1), ([10, 14, 3, 88, 24, 8, 158, 318, 5, 2, 6064, 12, 14409, 2, 961, 5, 0, 8, 3, 1333, 11, 7, 168, 44886, 5, 92, 46459, 34, 2, 29343, 12, 104671, 3, 88, 100, 1382, 14399, 11, 100, 153, 12790, 322, 130, 15, 2, 32900, 5, 16879, 31, 2751, 2, 4031, 5, 3564, 4, 3361, 1768, 60550, 59993, 2, 132989, 5, 418, 8, 1403, 5, 59, 0, 92282, 2, 37268, 8194, 5, 5707, 4, 38, 774, 29, 2, 657, 5, 2, 24, 7, 3, 1465, 120, 75, 11, 254, 675, 130, 8483, 5, 2, 1333, 63, 2, 103, 4213, 153, 2392, 2, 24, 7, 10064, 4, 19067, 302, 1157, 34, 505, 6, 656, 1030, 78, 3184, 7144, 2, 258, 8293, 5, 2, 24, 15, 9, 200, 12, 14, 1263, 11, 2, 103, 2545, 268, 1422, 6, 30, 167, 23, 2679, 31, 3, 1393, 15820, 16, 233, 80, 15, 76, 15, 2, 231, 11, 33, 22, 1499, 8, 26704, 16879, 17, 60, 2209, 4, 60, 80763, 10, 186, 304, 3, 182, 8, 2, 810, 8706, 5, 2, 646, 18, 78, 89, 1499, 8, 2, 163, 1173, 74, 23, 33671, 0, 12, 175, 827, 34, 2, 505, 5, 2, 24, 11, 2, 103, 121, 22, 168, 16049, 322, 50, 20940, 114, 42, 150, 29, 2, 172, 5, 2, 20, 17, 0, 53, 146, 12, 168, 827, 11, 41096, 1941, 53, 14, 30531, 4, 2, 13046, 14, 11, 46, 27, 62, 9486, 767, 27, 90, 26, 15563, 42, 168, 17836, 4, 81, 50, 0, 401, 49, 1084, 50, 42, 828, 401, 6, 23, 37, 717, 130, 1718, 85, 23, 453, 89, 72, 31404, 322, 33, 186, 2822, 5803, 8, 498, 8, 2, 163, 14511, 18, 0, 24, 12, 7, 23, 29, 35, 95, 827, 11, 33, 22, 17280, 34, 2, 505, 322, 168, 2, 8308, 45, 89, 22, 0, 12, 7, 2698, 18, 78, 2196, 6, 28, 452, 0, 40, 40327, 8, 2, 529, 5, 2, 2391, 8, 2, 151, 193, 62, 22137, 17802, 16, 7765, 421, 421, 421, 46, 10, 62, 23, 71, 2, 1707, 54, 33, 26, 3186, 236, 2, 311, 6, 2790, 59, 1941, 8, 3, 667, 11, 33, 186, 26, 1181, 23, 26, 0, 2, 24, 7891, 464, 2, 80, 2870, 7, 2, 13748, 5, 2, 121, 421, 421, 421, 2, 1652, 0, 4, 20940, 268, 3013, 4182, 6, 1787, 4, 15326, 31987, 3, 24, 8, 1416, 5236, 3, 1099, 667, 187, 3, 160, 699, 66871, 4, 167, 699, 0, 31, 695, 1652, 3409, 17, 898, 0, 68086, 2, 24, 7, 566, 221, 6, 814, 19, 83, 209, 4, 676, 60, 130, 7113, 8, 57, 3462, 29, 92, 8513, 2, 8293, 85, 23, 180, 76, 285, 34, 2, 818, 5, 2, 117, 12, 7, 1265, 3, 49, 1465, 4, 1057, 75, 322, 2, 1385, 4, 121, 74, 758, 17, 25, 3, 229, 84, 94, 25, 536, 2, 10270], 1), ([3, 6550, 7527, 151, 17, 4097, 590, 3932, 2488, 4, 60436, 13, 742, 45757, 554, 1116, 161, 23, 48, 53, 5868, 4, 458, 6, 1083, 42, 5707, 6, 907, 4472, 42, 5707, 6, 3814, 373, 17, 42, 1026, 0, 53, 146, 45, 7399, 88965, 34, 4507, 841, 4, 0], 1)]\n"
     ]
    }
   ],
   "source": [
    "# 把语料转换为id序列\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    data_set = []\n",
    "    for sentence, sentence_label in corpus:\n",
    "        # 将句子中的词逐个替换成id，如果句子中的词不在词表内，则替换成oov\n",
    "        # 这里需要注意，一般来说我们可能需要查看一下test-set中，句子oov的比例，\n",
    "        # 如果存在过多oov的情况，那就说明我们的训练数据不足或者切分存在巨大偏差，需要调整\n",
    "        sentence = [word2id_dict[word] if word in word2id_dict \\\n",
    "                    else word2id_dict['[oov]'] for word in sentence]    \n",
    "        data_set.append((sentence, sentence_label))\n",
    "    return data_set\n",
    "\n",
    "train_corpus = convert_corpus_to_id(train_corpus, word2id_dict)\n",
    "test_corpus = convert_corpus_to_id(test_corpus, word2id_dict)\n",
    "print(\"%d tokens in the corpus\" % len(train_corpus))\n",
    "print(train_corpus[:5])\n",
    "print(test_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们就可以开始把原始语料中的每个句子通过截断和填充，转换成一个固定长度的句子，并将所有数据整理成mini-batch，用于训练模型，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [1]]))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [1]]))\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [1]]))\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [1]]))\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [0]]))\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [0]]))\r"
     ]
    }
   ],
   "source": [
    "# 编写一个迭代器，每次调用这个迭代器都会返回一个新的batch，用于训练或者预测\n",
    "def build_batch(word2id_dict, corpus, batch_size, epoch_num, max_seq_len, shuffle = True, drop_last = True):\n",
    "\n",
    "    # 模型将会接受的两个输入：\n",
    "    # 1. 一个形状为[batch_size, max_seq_len]的张量，sentence_batch，代表了一个mini-batch的句子。\n",
    "    # 2. 一个形状为[batch_size, 1]的张量，sentence_label_batch，每个元素都是非0即1，代表了每个句子的情感类别（正向或者负向）\n",
    "    sentence_batch = []\n",
    "    sentence_label_batch = []\n",
    "\n",
    "    for _ in range(epoch_num): \n",
    "\n",
    "        #每个epoch前都shuffle一下数据，有助于提高模型训练的效果\n",
    "        #但是对于预测任务，不要做数据shuffle\n",
    "        if shuffle:\n",
    "            random.shuffle(corpus)\n",
    "\n",
    "        for sentence, sentence_label in corpus:\n",
    "            sentence_sample = sentence[:min(max_seq_len, len(sentence))]\n",
    "            if len(sentence_sample) < max_seq_len:\n",
    "                for _ in range(max_seq_len - len(sentence_sample)):\n",
    "                    sentence_sample.append(word2id_dict['[pad]'])\n",
    "            \n",
    "            \n",
    "            sentence_sample = [[word_id] for word_id in sentence_sample]\n",
    "\n",
    "            sentence_batch.append(sentence_sample)\n",
    "            sentence_label_batch.append([sentence_label])\n",
    "\n",
    "            if len(sentence_batch) == batch_size:\n",
    "                yield np.array(sentence_batch).astype(\"int64\"), np.array(sentence_label_batch).astype(\"int64\")\n",
    "                sentence_batch = []\n",
    "                sentence_label_batch = []\n",
    "        if not drop_last and len(sentence_batch) > 0:\n",
    "            yield np.array(sentence_batch).astype(\"int64\"), np.array(sentence_label_batch).astype(\"int64\")\n",
    "\n",
    "for batch_id, batch in enumerate(build_batch(word2id_dict, train_corpus, batch_size=3, epoch_num=3, max_seq_len=30)):\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络定义\n",
    "\n",
    "在讲解卷积神经网络的的章节，我们详细列出了每一种神经网络使用基础算子拼装的详细网络配置，但实际上对于一些常用的网络结构，飞桨框架提供了现成的中高层函数支持。下面用于情感分析的长短时记忆模型就使用 [paddle.nn.LSTM](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/api/paddle/nn/layer/rnn/LSTM_cn.html) API实现。如果读者对使用基础算子拼装LSTM的内容感兴趣，可以查阅[paddle.nn.LSTM](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/api/paddle/nn/layer/rnn/LSTM_cn.html) 类的源代码。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddle.nn import LSTM, Embedding, Dropout, Linear\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# 定义一个用于情感分类的网络实例，SentimentClassifier\n",
    "class SentimentClassifier(paddle.nn.Layer):\n",
    "    def __init__(self, hidden_size, vocab_size, class_num=2, num_steps=128, num_layers=1, init_scale=0.1, dropout=None):\n",
    "        \n",
    "        # 参数含义如下：\n",
    "        # 1.hidden_size，表示embedding-size，hidden和cell向量的维度\n",
    "        # 2.vocab_size，模型可以考虑的词表大小\n",
    "        # 3.class_num，情感类型个数，可以是2分类，也可以是多分类\n",
    "        # 4.num_steps，表示这个情感分析模型最大可以考虑的句子长度\n",
    "        # 5.num_layers，表示网络的层数\n",
    "        # 6.init_scale，表示网络内部的参数的初始化范围\n",
    "        # 长短时记忆网络内部用了很多Tanh，Sigmoid等激活函数，这些函数对数值精度非常敏感，\n",
    "        # 因此我们一般只使用比较小的初始化范围，以保证效果\n",
    "\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.class_num = class_num\n",
    "        self.init_scale = init_scale\n",
    "        self.num_layers = num_layers\n",
    "        self.num_steps = num_steps\n",
    "        self.dropout = dropout\n",
    "       \n",
    "        # 声明一个LSTM模型，用来把每个句子抽象成向量\n",
    "        self.simple_lstm_rnn = LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "        # 声明一个embedding层，用来把句子中的每个词转换为向量\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size, sparse=False, \n",
    "                                    weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Uniform(low=-init_scale, high=init_scale)))\n",
    "        \n",
    "        # 在得到一个句子的向量表示后，需要根据这个向量表示对这个句子进行分类\n",
    "        # 一般来说，可以把这个句子的向量表示乘以一个大小为[self.hidden_size, self.class_num]的W参数，\n",
    "        # 并加上一个大小为[self.class_num]的b参数，从而达到把句子向量映射到分类结果的目的\n",
    "        \n",
    "        # 我们需要声明最终在使用句子向量映射到具体情感类别过程中所需要使用的参数\n",
    "        # 这个参数的大小一般是[self.hidden_size, self.class_num]\n",
    "        self.cls_fc = Linear(in_features=self.hidden_size, out_features=self.class_num, \n",
    "                             weight_attr=None, bias_attr=None)\n",
    "        self.dropout_layer = Dropout(p=self.dropout, mode='upscale_in_train')\n",
    "\n",
    "    def forward(self, input, label):\n",
    "\n",
    "        # 首先我们需要定义LSTM的初始hidden和cell，这里我们使用0来初始化这个序列的记忆\n",
    "        init_hidden_data = np.zeros(\n",
    "            (self.num_layers, batch_size, embedding_size), dtype='float32')\n",
    "        init_cell_data = np.zeros(\n",
    "            (self.num_layers, batch_size, embedding_size), dtype='float32')\n",
    "\n",
    "        # 将这些初始记忆转换为飞桨可计算的向量\n",
    "        # 设置stop_gradient=True，避免这些向量被更新，从而影响训练效果\n",
    "        init_hidden = paddle.to_tensor(init_hidden_data)\n",
    "        init_hidden.stop_gradient = True\n",
    "        init_cell = paddle.to_tensor(init_cell_data)\n",
    "        init_cell.stop_gradient = True\n",
    "\n",
    "        init_h = paddle.reshape(\n",
    "            init_hidden, shape=[self.num_layers, -1, self.hidden_size])\n",
    "        init_c = paddle.reshape(\n",
    "            init_cell, shape=[self.num_layers, -1, self.hidden_size])\n",
    "\n",
    "        # 将输入的句子的mini-batch转换为词向量表示\n",
    "        x_emb = self.embedding(input)\n",
    "        x_emb = paddle.reshape(\n",
    "            x_emb, shape=[-1, self.num_steps, self.hidden_size])\n",
    "        if self.dropout is not None and self.dropout > 0.0:\n",
    "            x_emb = self.dropout_layer(x_emb)\n",
    "        \n",
    "        # 使用LSTM网络，把每个句子转换为向量表示\n",
    "        rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb, (init_h, init_c))\n",
    "        last_hidden = paddle.reshape(\n",
    "            last_hidden[-1], shape=[-1, self.hidden_size])\n",
    "\n",
    "        # 将每个句子的向量表示映射到具体的情感类别上\n",
    "        projection = self.cls_fc(last_hidden)\n",
    "        pred = F.softmax(projection, axis=-1)\n",
    "        \n",
    "        # 根据给定的标签信息，计算整个网络的损失函数，这里我们可以直接使用分类任务中常使用的交叉熵来训练网络\n",
    "        loss = F.softmax_with_cross_entropy(\n",
    "            logits=projection, label=label, soft_label=False)\n",
    "        loss = paddle.mean(loss)\n",
    "\n",
    "        # 最终返回预测结果pred，和网络的loss\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "\n",
    "在完成模型定义之后，我们就可以开始训练模型了。当训练结束以后，我们可以使用测试集合评估一下当前模型的效果，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "batch_size = 128\n",
    "epoch_num = 5\n",
    "embedding_size = 256\n",
    "learning_rate = 0.01\n",
    "max_seq_len = 128\n",
    "\n",
    "# 使用GPU\n",
    "paddle.set_device('gpu:0')\n",
    "\n",
    "def train():\n",
    "    step = 0\n",
    "    sentiment_classifier = SentimentClassifier(\n",
    "            embedding_size, vocab_size, num_steps=max_seq_len, num_layers=1)\n",
    "    # 创建优化器Optimizer，用于更新这个网络的参数        \n",
    "    optimizer = paddle.optimizer.Adam(learning_rate=0.01, beta1=0.9, beta2=0.999, parameters= sentiment_classifier.parameters()) \n",
    "\n",
    "    sentiment_classifier.train()\n",
    "    for sentences, labels in build_batch(\n",
    "        word2id_dict, train_corpus, batch_size, epoch_num, max_seq_len):\n",
    "        \n",
    "        sentences_var = paddle.to_tensor(sentences)\n",
    "        labels_var = paddle.to_tensor(labels)\n",
    "        pred, loss = sentiment_classifier(sentences_var, labels_var)\n",
    "        \n",
    "        # 后向传播\n",
    "        loss.backward()\n",
    "        # 最小化loss\n",
    "        optimizer.step()\n",
    "        # 清除梯度\n",
    "        optimizer.clear_grad()\n",
    "        \n",
    "        step += 1\n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\n",
    "                \n",
    "    # 我们希望在网络训练结束以后评估一下训练好的网络的效果\n",
    "    # 通过eval()函数，将网络设置为eval模式，在eval模式中，网络不会进行梯度更新\n",
    "    eval(sentiment_classifier)\n",
    "\n",
    "def eval(sentiment_classifier):\n",
    "    sentiment_classifier.eval()\n",
    "    # 这里我们需要记录模型预测结果的准确率\n",
    "    # 对于二分类任务来说，准确率的计算公式为：\n",
    "    # (true_positive + true_negative) / \n",
    "    # (true_positive + true_negative + false_positive + false_negative)\n",
    "    tp = 0.\n",
    "    tn = 0.\n",
    "    fp = 0.\n",
    "    fn = 0.\n",
    "    for sentences, labels in build_batch(\n",
    "        word2id_dict, test_corpus, batch_size, 1, max_seq_len):\n",
    "        \n",
    "        sentences_var = paddle.to_tensor(sentences)\n",
    "        labels_var = paddle.to_tensor(labels)\n",
    "        \n",
    "        # 获取模型对当前batch的输出结果\n",
    "        pred, loss = sentiment_classifier(sentences_var, labels_var)\n",
    "\n",
    "        # 把输出结果转换为numpy array的数据结构\n",
    "        # 遍历这个数据结构，比较预测结果和对应label之间的关系，并更新tp，tn，fp和fn\n",
    "        pred = pred.numpy()\n",
    "        for i in range(len(pred)):\n",
    "            if labels[i][0] == 1:\n",
    "                if pred[i][1] > pred[i][0]:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            else:\n",
    "                if pred[i][1] > pred[i][0]:\n",
    "                    fp += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "\n",
    "    # 输出最终评估的模型效果\n",
    "    print(\"the acc in the test set is %.3f\" % ((tp + tn) / (tp + tn + fp + fn)))\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本匹配\n",
    "\n",
    "借助相同的思路，我们可以很轻易的解决文本相似度计算问题，假设给定两个句子：\n",
    "\n",
    "> 句子1：我不爱吃烤冷面，但是我爱吃冷面\n",
    ">\n",
    "> 句子2：我爱吃菠萝，但是不爱吃地瓜\n",
    "\n",
    "同样使用LSTM网络，把每个句子抽象成一个向量表示，通过计算这两个向量之间的相似度，就可以快速完成文本相似度计算任务。在实际场景里，我们也通常使用LSTM网络的最后一步hidden结果，将一个句子抽象成一个向量，然后通过向量点积，或者cosine相似度的方式，去衡量两个句子的相似度。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/a631a1541a354f7ea26f54b35853469fa6cd56ca5b884372b7e53a4a8657c7ad\" width=\"600\" ></center>\n",
    "<br><center>图9：文本相似度计算</center></br>\n",
    "\n",
    "一般情况下，在训练阶段有point-wise和pair-wise两个常见的训练模式（针对搜索引擎任务，还有一类list-wise的方法，这里不做探讨）。\n",
    "* **point-wise训练模式：** 在point-wise训练过程中，我们把不同的句子对儿分为两类（或者更多类别）：相似、不相似。通过这种方式把句子相似度计算任务转化为了一个分类问题，通过常见的二分类函数（如sigmoid）即可完成分类任务。在最终预测阶段，使用sigmoid函数的输出，作为两个不同句子的相似度值。\n",
    "\n",
    "* **pair-wise训练模式：** pair-wise训练模式相对更复杂一些，假定给定3个句子，A，B和C。已知A和B相似，但是A和C不相似，那么原则上，A和B的相似度值应该高于A和C的相似度值。因此我们可以构造一个新的训练算法：对于一个相同的相似度计算模型$m$，假定$m(A,B)$是$m$输出的$A$和$B$的相似度值，$m(A,C)是$m$输出的$A$和$C$的相似度值，那么hinge-loss：\n",
    "\n",
    "$L = \\lambda - (m(A,B)-m(A,C))$ if $m(A,B)-m(A,C) < \\lambda$ else $0$\n",
    "\n",
    "这个损失函数要求对于每个正样本$m(A,B)$的相似度值至少高于负样本$m(A,C)$一个阈值$\\lambda$。\n",
    "\n",
    "hinge-loss的好处是没有强迫进行单个样本的分类，而是通过考虑样本和样本直接的大小关系来学习相似和不相似关系。相比较而言，pair-wise训练比point-wise任务效果更加鲁棒一些，更适合如搜索，排序，推荐等场景的相似度计算任务。\n",
    "\n",
    "有兴趣的读者可以参考情感分析的模型实现，自行实现一个point-wise或pair-wise的文本相似度模型，相关数据集可参考[文本匹配数据集](https://aistudio.baidu.com/aistudio/datasetdetail/12739)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思考一下\n",
    "\n",
    "[1] 情感分析任务对你有什么启发？\n",
    "\n",
    "[2] 除了LSTM，你还能想到那些其他方法，构造一个句子的向量表示？\n",
    "\n",
    "[3] 对一个句子生成一个单一的向量表示有什么缺点，你还知道其他方式吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引用\n",
    "\n",
    "[1] [维基百科：情感分析](https://zh.wikipedia.org/zh-hans/文本情感分析)\n",
    "\n",
    "[2] [维基百科：RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "\n",
    "[3] [维基百科：LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "\n",
    "[4] [bilibili：肥伦秀](https://www.bilibili.com/video/av40396494?from=search&seid=9852893210841347755)\n",
    "\n",
    "[5] [知乎：GIF动图一步一步看懂LSTM和GRU](https://zhuanlan.zhihu.com/p/81549798)\n",
    "\n",
    "[6] [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
